{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AICrossSim/NewComputeBench","text":"NewComputeBench <p>AICrossSim/NewComputeBench is a benchmark suite for new compute paradigms (Spiking neural networks, Optical computation, In-Memory computation, etc) via software emulation. We aim to predict the scaling law of neural networks trained with new compute paradigms by running small &amp; medium scale experiments and extrapolate the trends we observed. NewComputeBench project mainly consists of the following steps:</p> <ul> <li> Build a scaling framework to support the pre-training of language models up to 1.1B parameters (CLM model series)</li> <li> Implement software emulation of new compute paradigms (e.g., optical compute, spiking neural networks, in-memory compute, etc)</li> <li> Filter out promising new compute paradigms by running small &amp; medium scale experiments (Roberta on GLUE)</li> <li> Scale up the promising new compute paradigms to large-scale language models</li> <li> Fine-tuning/pretraining of CLM models (60M - 1.1B)<ul> <li> Optical compute</li> <li> Spiking neural networks</li> <li> In-memory compute</li> </ul> </li> <li> Parameter-efficient fine-tuning of larger LLMs (e.g., Llama-3.1-8B)<ul> <li> Optical compute (failed to converge)</li> </ul> </li> </ul>"},{"location":"#whats-new","title":"What's New","text":"<ul> <li> <p>\ud83d\udea74th Oct, 2025 Milestone: Fine-tuning/pretraining of alternative compute paradigms on CLMs.</p> Item Description Optical Transformer Tutorial </li> <li> <p>\ud83d\udea91th Oct, 2025 Milestone: Fine-tuning/pretraining of alternative compute paradigms on Roberta</p> Item Description Optical Transformer Tutorial CompleteThis </li> <li> <p>\ud83d\udea9 9th, Jun, 2025 Milestone: Our Software-emulation &amp; acceleration backend, Mase-triton, is released on PyPI. Try it via <code>pip install mase-triton</code>.</p> <ul> <li>For more details, please refer to Intro to Mase-triton and Mase-triton GitHub</li> </ul> </li> <li> <p>\ud83d\udea9 15th April, 2025 Milestone: System and model-level training simulation (Small Language Models).</p> Item Description Environment setup Tutorial Pretraining AICrossSim LLMs (60M, 200M, 400M, 1.1B) &amp; evaluation Tutorial Software-emulated bitflip-aware pretraining &amp; evaluation Tutorial </li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Model Training &amp; Evaluation<ul> <li>LLMs<ul> <li> Pretraining of LLMs (60M, 200M, 400M, 1.1B) using the Llama-3 architecture.</li> <li> <code>lm-eval-harness</code> evaluation of LLMs.</li> <li> Parameter-efficient fine-tuning</li> <li> Supervised fine-tuning</li> </ul> </li> </ul> </li> <li>Model Behavior-Level Simulation<ul> <li> Post-training bitflip transform &amp; bitflip-aware pretraining</li> <li> Optical compute<ul> <li> Roberta</li> <li> CLM</li> </ul> </li> <li> Spiking neural networks</li> <li> In-memory compute</li> </ul> </li> </ul>"},{"location":"#about-the-project","title":"About the Project","text":"<p>This project is led by Dr. Yiren Zhao at Imperial College London, Dr. Luo Mai at University of Edinburgh, Prof. Robert Mullins at University of Cambridge, and funded by Advanced Research + Invention Agency (ARIA).</p>"},{"location":"dev-guide/","title":"Developer Guide","text":""},{"location":"dev-guide/#environment-setup","title":"Environment Setup","text":"<p>Besides the environment setup in Environment Setup, you need to install <code>mkdocs-material</code> for maintaining the documentation.</p> <pre><code>pip install mkdocs-material mkdocs-git-revision-date-localized-plugin mkdocs-git-committers-plugin-2\n</code></pre>"},{"location":"dev-guide/#documentation","title":"Documentation","text":"<p>Currently we maintain the deliverable documentation in <code>docs</code> folder. The documentation is built using MkDocs and Material for MkDocs, and each markdown file is a page in the documentation. Everytime a new commit is pushed to the <code>main</code> branch, the documentation will be automatically built and deployed to GitHub Pages.</p>"},{"location":"dev-guide/#how-to-add-a-new-page","title":"How to add a new page?","text":"<ol> <li>Create a new markdown file in the <code>docs</code> folder.</li> <li>Add the new page to the <code>mkdocs.yml</code> file under the <code>nav</code> section.</li> </ol>"},{"location":"dev-guide/#how-to-preview-the-documentation","title":"How to preview the documentation?","text":"<p>Run the following command in the root directory of the project:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then you can preview the static sites in your browser.</p>"},{"location":"env-setup/","title":"Environment Setup","text":""},{"location":"env-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux or WSL2</li> <li>CUDA-enabled GPU</li> <li>MiniConda or Anaconda (for installing Cuda-Toolkit)</li> </ul> <p>Our Environment Setup for Reference</p> <p>We run all the experiments on linux machines and did not test on Windows.</p> <p>Here are a few environment we have tested for reference:</p> <ul> <li>NVIDIA A6000 48GBx8, Ubuntu 24.04, CUDA 12.4</li> <li>NVIDIA H100 96GBx2, Red Hat Enterprise Linux 9.5, CUDA 12.6.</li> <li>NVIDIA H100 80GBx8, Ubuntu 24.04, CUDA 12.4</li> <li>NVIDIA H200 141GBx8, Ubuntu 24.04, CUDA 12.4</li> </ul>"},{"location":"env-setup/#environment-setup_1","title":"Environment Setup","text":"<ol> <li> <p>Config SSH key for GitHub. One of the dependencies, MASE, requires SSH to clone and install. Please set up <code>~/.ssh/config</code> accordingly (refer to Connecting to GitHub with SSH).</p> </li> <li> <p>Clone the project repository</p> <pre><code>git clone https://github.com/AICrossSim/NewComputeBench.git\ncd NewComputeBench\ngit submodule update --init\n</code></pre> </li> <li> <p>Create a new conda environment</p> <pre><code>conda env create -f environment.yaml\n</code></pre> </li> <li> <p>Activate the new environment and install required packages</p> <pre><code>conda activate new-compute\n</code></pre> <p>We recommend check if the python and pip in <code>$PATH</code> are from the conda environment: <pre><code>which python\nwhich pip\n</code></pre></p> <p>Then install the required packages: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>(Optional) You may want to log in Wandb to track the training logs.</p> <pre><code>wandb login\n</code></pre> </li> </ol>"},{"location":"model-list/","title":"Models","text":"<p>A list of models we aim to port to NewComputeBench.</p> Task Type Model Name Model Sizes Description Text classification <code>RoBERTa</code> <code>roberta-base</code> A classic encoder-only language model we include for sanity checks. Causal language modeling <code>AICrossSim-CLM</code> 60M, 200M, 400M, 1.1B A family of small language models using Llama-3.1 architecture.  We use <code>cosmo2-tokenizer</code> and pretrain them on Fineweb-Edu. Causal language modeling <code>Llama-3</code> 1B, 3B, 8B, 70B Meta's Llama-3 model family Causal language modeling TBD TBD TBD Image generation TBD TBD TBD Image classification TBD TBD TBD"},{"location":"model-list/#model-training","title":"Model Training","text":"<ul> <li> <p>Pretraining from scratch</p> Model Names Supported? <code>RoBERTa</code> \u2705 <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 </li> <li> <p>Fine-tuning</p> Model Names Supported? <code>RoBERTa</code> \u2705 <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f </li> <li> <p>Evaluation</p> Task Model Name Supported? Text classification (GLUE) <code>RoBERTa</code> \u2705 Causal language modeling <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 </li> </ul>"},{"location":"model-list/#model-behavior-level-simulation","title":"Model Behavior-Level Simulation","text":"<ul> <li> <p>Transform-aware pretraining from scratch</p> Transform Model Name Supported? Random Bitflip <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 Optical Compute <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f In-Memory Compute <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f Spiking Neural Networks <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f </li> <li> <p>Post-transform/training evaluation</p> Transform Task Model Name Supported? Random Bitflip Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f Optical Compute Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f In-Memory Compute Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f Spiking Neural Networks Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f </li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/","title":"LLM Pretraining","text":"<p>This is a tutorial on how to pretrain AICrossSim-CLM using NewComputeBench.</p>"},{"location":"01-model-training/llm-pretrain-and-eval/#overview","title":"Overview","text":"<ul> <li>We aim to pretrain AICrossSim-CLM models (60M, 200M, 400M, 1.1B) on the Fineweb-Edu dataset.<ul> <li>We followed the Chinchilla scaling law to determine the number of training tokens: <code>num_tokens = 22 * num_params</code>.</li> <li>As the model size increases, the training time and memory requirements will increase significantly. For example, we pretrained the 1.1B model on 8 NVIDIA H100 80GB GPUs for 1.4 days, while the 60M model can be pretrained on 2 NVIDIA H100 80GB GPUs within 1 hour.</li> </ul> </li> <li>The pretraining entrypoint is at <code>experiments/llm-digital/pretrain/run.py</code><ul> <li><code>run.py</code> supports multiple subcommands, including <code>pretrain</code>, <code>eval</code>, <code>generate-hf</code>, <code>convert-ckpt</code>, and <code>generate-cfg</code>.<ul> <li>Run <code>python run.py -h</code> to see the available subcommands.</li> <li>Run <code>python run.py &lt;subcommand&gt; -h</code> to see the help message for a specific subcommand.</li> </ul> </li> <li>To run distributed training, we use <code>torchrun</code> to launch the training script.</li> </ul> </li> <li>We uploaded the pretrained models to HuggingFace for easy access: NewComputeBench-CLM-Digital</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#pretraining","title":"Pretraining","text":"<p>Environment Setup?</p> <p>If you have not set up environments, please follow the guidelines in Environment Setup.</p>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-60m","title":"AICrossSim-CLM-60M","text":"<p>We demonstrate the pretraining process using the <code>AICrossSim-CLM-60M</code> model. The same process can be applied to other models with minor adjustments.</p> <ol> <li> <p>Change the working directory to <code>experiments/llm-digital/pretrain</code> and activate the conda environment.</p> <pre><code>cd experiments/llm-digital/pretrain\nconda activate new-compute\n</code></pre> </li> <li> <p>Generate pretraining config</p> <p>Fast Development Run?</p> <p><code>generate-cfg</code> has several default arguments. You may want to change them for a fast development run:</p> <ul> <li><code>--batch_size</code>: a smaller batch size to avoid out-of-memory errors.</li> <li><code>--data_parallel_replicate_degree</code>: partition the training data across multiple GPUs. Each GPU receives a subset of the training data.</li> <li><code>--data_parallel_shard_degree</code>: partition the model parameters across multiple GPUs. Each GPU receives a subset of the model parameters. Default <code>-1</code> means no sharding.</li> <li><code>--token_num_scale</code>: the scale used to determine the number of training tokens: <code>num_tokens = token_num_scale * num_params</code>, 22 by default. Set this to a small value like <code>1</code> to reduce the number of training steps.</li> </ul> <pre><code>data_parallel=\"2\"       # For Simplicity, we set this to number of GPUs per node\nbatch_size=\"48\"         # Per-device batch size\ntoken_num_scale=\"22\"    # Scale for number of training tokens\npython run.py generate-cfg \\\n    --model_flavor 60M --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel} \\\n    --compile true \\\n    --save_path ./configs/tutorial-60M.yaml\n</code></pre> <ul> <li>This will generate a training configuration file <code>configs/tutorial-60M.yaml</code> for pretraining <code>AICrossSim-CLM-60M</code> model using a per-device batch size of 48 and a data parallel replicate degree of 2 on a FineWeb-Edu subset of <code>22 * 60M</code> tokens.</li> <li>Subcommand <code>generate-cfg</code> automatically calculates the number of training steps.</li> <li>The <code>--compile</code> flag enables the use of <code>torch.compile</code> for optimizing the training process.</li> </ul> </li> <li> <p>Launch pretraining</p> <pre><code>num_gpus=\"2\" # Number of GPUs per node. We only use one node for this example\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-60M.yaml \\\n    --metrics_args.enable_wandb false # disable wandb in case the user does not log in wandb\n</code></pre> <ul> <li>This will pass the generated configuration file and launch the pretraining job on a single node of 2 GPUs using <code>torchrun</code>.</li> <li>The <code>--metrics_args.enable_wandb</code> flag disables Weights and Biases logging. You can enable it by setting it to <code>true</code>.</li> <li>The <code>STREAM_HF_DATA</code> environment variable is set to <code>1</code> to enable streaming data loading from Hugging Face datasets instead of downloading the huge dataset to the local disk.</li> <li>When the training is finished, the model checkpoint will be saved at <code>./outputs/checkpoints/aixsim-60M/&lt;timestamp&gt;</code>.</li> </ul> Fatal Python error: Aborted ? <p>We noticed that after the training is finished, <code>torchrun</code> may raise the error \"Fatal Python error: Aborted\" when destroying process group. This does not affect the training results as long as the error is raised after the final checkpoint is saved (messages like \"[rank0]:2025-04-01 00:25:59,616 - root - INFO - Finished saving the checkpoint (or staging if async is enabled)in 5.53 seconds.\")</p> <p>Here is an example of the error message:</p> <pre><code>[rank0]:2025-04-01 00:25:47,084 - root - INFO - step: 640/644 = 99.3789%  loss:  6.2116  memory: 87.04GiB(93.48%)  tps: 52,142  mfu: 3.09%\n[rank0]:2025-04-01 00:25:54,090 - root - INFO - Saving a full checkpoint at last step, step 644.\n[rank0]:2025-04-01 00:25:59,616 - root - INFO - Finished saving the checkpoint (or staging if async is enabled)in 5.53 seconds.\n[rank0]:2025-04-01 00:25:59,617 - root - INFO - Sleeping 2 seconds for other ranks to complete\n[rank0]:2025-04-01 00:26:01,706 - root - INFO - Training completed\n[rank0]:terminate called without an active exception\n[rank0]:Fatal Python error: Aborted\n[rank0]:\n[rank0]:Current thread 0x00007f0360ff9640 (most recent call first):\n[rank0]:  Garbage-collecting\n[rank0]:  &lt;no Python frame&gt;\n[rank0]:\n[rank0]:Thread 0x00007f06b6639200 (most recent call first):\n[rank0]:  &lt;no Python frame&gt;\nW0401 00:26:06.901000 3016633 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3016738 closing signal SIGTERM\nE0401 00:26:08.212000 3016633 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 1 (pid: 3016739) of binary: /home/zz7522/miniconda3/envs/new-compute/bin/python3.11\nTraceback (most recent call last):\nFile \"/home/zz7522/miniconda3/envs/new-compute/bin/torchrun\", line 8, in &lt;module&gt;\n    sys.exit(main())\n            ^^^^^^\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n        ^^^^^^^^^^^^^^^^^^\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/run.py\", line 918, in main\n    run(args)\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/run.py\", line 909, in run\n    elastic_launch(\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n========================================================\nrun.py FAILED\n--------------------------------------------------------\nFailures:\n&lt;NO_OTHER_FAILURES&gt;\n--------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\ntime      : 2025-04-01_00:26:06\nhost      : ee-tiamat.ee.ic.ac.uk\nrank      : 1 (local_rank: 1)\nexitcode  : -6 (pid: 3016739)\nerror_file: &lt;N/A&gt;\ntraceback : Signal 6 (SIGABRT) received by PID 3016739\n========================================================\n</code></pre> </li> <li> <p>(Optional) Convert to HuggingFace checkpoint</p> <p>HuggingFace Checkpoint</p> <p>To support distributed training, the training code defines custom model classes, and the checkpoints are saved in a custom format by <code>torchrun</code>. To exploit the HuggingFace ecosystem, we provide a script to convert the custom checkpoint to HuggingFace format.</p> <pre><code>python run.py convert-ckpt aixsim 60M \\\n    ./outputs/checkpoints/aixsim-60M/&lt;timestamp&gt;/&lt;step-xxx&gt; \\\n    path/to/huggingface/checkpoint\n</code></pre> </li> </ol> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-60M</code> model on 2 NVIDIA H100 96GB GPUs for 1 hour.</p> <ul> <li>Training config: <code>experiments/llm-digital/pretrain/configs/aixsim-60M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-60m</li> </ul> <p>Similarly, you can pretrain the other models by changing the <code>--model_flavor</code> argument to <code>200M</code>, <code>400M</code>, or <code>1.1B</code>, and adjusting <code>--batch_size</code>, <code>--data_parallel_replicate_degree</code>, <code>--data_parallel_shard_degree</code>, and <code>--token_num_scale</code> accordingly.</p>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-200m","title":"AICrossSim-CLM-200M","text":"<p>We applied Fully Sharded Data Parallel (FSDP) to the <code>AICrossSim-CLM-200M</code> training job to reduce memory usage, but this increases the training time.</p> <pre><code>batch_size=\"32\"\ndata_parallel_replicate=\"1\"\ndata_parallel_shard=\"2\"\npython run.py generate-cfg \\\n    --model_flavor 200M --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel_replicate} \\\n    --data_parallel_shard_degree ${data_parallel_shard} \\\n    --compile true \\\n    --save_path ./configs/tutorial-200M.yaml\n\nnum_gpus=\"2\" # 2 GPUs, 1 node\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-200M.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-200M</code> model on 2 NVIDIA H100 96GB GPUs for 6.5 hours.</p> <ul> <li>Training config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-200M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-200m</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-400m","title":"AICrossSim-CLM-400M","text":"<pre><code>batch_size=\"12\"\ndata_parallel_replicate=\"1\"\ndata_parallel_shard=\"8\"\npython run.py generate-cfg \\\n    --model_flavor 400M --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel_replicate} \\\n    --data_parallel_shard_degree ${data_parallel_shard} \\\n    --compile true \\\n    --save_path ./configs/tutorial-400M.yaml\n\nnum_gpus=\"8\" # 8 GPUs, 1 node\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-400M.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-400M</code> model on 8 NVIDIA A6000 GPUs for 21 hours.</p> <ul> <li>Training config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-200M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-400m</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-11b","title":"AICrossSim-CLM-1.1B","text":"<pre><code>batch_size=\"24\"\ndata_parallel_replicate=\"1\"\ndata_parallel_shard=\"8\"\npython run.py generate-cfg \\\n    --model_flavor 1.1B --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel_replicate} \\\n    --data_parallel_shard_degree ${data_parallel_shard} \\\n    --compile true \\\n    --save_path ./configs/tutorial-1.1B.yaml\n\nnum_gpus=\"8\" # 8 GPUs, 1 node\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-1.1B.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-1.1B</code> model on 8 NVIDIA H100 96GB GPUs for 33 hours.</p> <ul> <li>Training config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-1.1b.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-1.1b</li> <li>We also stored the raw torchrun checkpoints in the HuggingFace repo in case we need to resume pretraining later. You can find them here</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#evaluation","title":"Evaluation","text":""},{"location":"01-model-training/llm-pretrain-and-eval/#pretraining-dataset-perplexity","title":"Pretraining Dataset Perplexity","text":"<p>We provide subcommands to evaluate the torchrun or HuggingFace checkpoints on the pretraining dataset.</p> <ul> <li>Torchrun checkpoint     <pre><code>python run.py eval pt-ppl \\\n    aixsim 60M \\\n    ./outputs/checkpoints/aixsim-60M/&lt;timestamp&gt;/&lt;step-xxx&gt;  # path to torchrun checkpoint\n</code></pre></li> <li>HuggingFace checkpoint     <pre><code>python run.py eval hf-ppl \\\n    AICrossSim/clm-60m  # path to local HuggingFace checkpoint or HuggingFace repo name\n</code></pre></li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#downstream-tasks","title":"Downstream Tasks","text":"<p>We leverage <code>lm-eval-harness</code> to evaluate the pretrained models on various tasks.</p> <p>For example, <pre><code>model_name=\"AICrossSim/clm-60m\" # Path to local HuggingFace checkpoint or HuggingFace repo name\npython run.py eval hf-lm-eval \\\n    ${model_name} \\\n    --tasks ['wikitext'] \\\n    --dtype float16\n</code></pre></p> <p>Try <code>--help</code> to see all the available arguments.</p> <pre><code>python run.py hf-lm-eval -h\n</code></pre> <p><code>lm-eval-harness</code></p> <p>Under the hood, the subcommand <code>hf-lm-eval</code> uses <code>lm-eval-harness</code>'s <code>simple_evaluate</code> function, thus it accepts several arguments of <code>simple_evaluate</code>:</p> <ul> <li><code>--tasks</code>: a list of tasks to evaluate on. The task names are the same as those in <code>lm-eval-harness</code>.</li> <li><code>--num_fewshot</code>: some downstream tasks support few-shot evaluation. Default <code>None</code> means default few-shot setting.</li> <li><code>--limit</code>: If <code>--limit</code> &gt; 1, it's the maximum number of examples to evaluate on, else it denotes the fraction of the dataset to evaluate on. Default <code>None</code> means evaluate on the entire dataset.</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#simple-generation","title":"Simple Generation","text":"<p>We also provide a simple generation subcommand <code>hf-gen</code> to generate text using the pretrained models.</p> <pre><code>prompt=\"London is\"\nmax_new_tokens=\"100\"\ndo_sample=\"true\"\ntemperature=\"0.6\"\ntop_k=\"50\"\ntop_p=\"0.9\"\n\npython run.py hf-gen \\\n    --model_name AICrossSim/clm-60m \\\n    --prompt \"${prompt}\" \\\n    --max_new_tokens ${max_new_tokens} \\\n    --do_sample ${do_sample} \\\n    --temperature ${temperature} \\\n    --top_k ${top_k} \\\n    --top_p ${top_p}\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/","title":"Random Bitflip","text":"<p>This is tutorial on how to run post-bitflip evaluation on a pretrained checkpoint, and how to run a bitflip-aware pretraining from scratch.</p>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/#overview","title":"Overview","text":"<ul> <li>Post-bitflip evaluation loads a pretrained checkpoint from HuggingFace, applies bitflip transformation to the model, and evaluates the model on downstream tasks.<ul> <li>The entry point is at <code>experiments/llm-bitflip/transform/minimal.py</code>.</li> </ul> </li> <li>Bitflip-aware pretraining creates a randomly initialized model, applies bitflip transformation to the model, and pretrains the model on FineWeb-Edu.<ul> <li>The entry point is at <code>experiments/llm-bitflip/pretrain/run.py</code>.</li> </ul> </li> <li>To accelerate the emulation, we build custom triton kernels in <code>mase-triton</code>.<ul> <li>The random bitflip kernel is wrapped in function <code>mase_triton.random_bitflip.core.random_bitflip_fn</code>, which supports unique bitflip probability for the sign-exp bits and the mantissa bits. <code>random_bitflip_fn</code> also supports zeroing out outliers (and \"NaN\" values) by assigning a threshold.</li> <li>The random bitflip probability only supports a power of 0.5, e.g, <code>0.5</code>, <code>0.5^2</code>, <code>0.5^3</code>, etc. The kernel will automatically convert the probability to the nearest power of 0.5. Due to the limitation of the pseudo random number generation algorithm (Philox), the kernel only works for a random bitflip probability greater or equal to <code>0.5^-24=5.96-08</code>.</li> </ul> </li> </ul>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/#evaluation-of-post-training-bitflip-transform","title":"Evaluation of Post-Training Bitflip Transform","text":"<p>Environment Setup?</p> <p>If you have not set up environments, please follow the guidelines in Environment Setup.</p> <p>We offer minimal scripts to apply post-training bitflip transform on all linear layers (contributing to over 90% FLOPS in Transformers) in a HuggingFace pretrained model and evaluate the transformed model with <code>lm-eval-harness</code>.</p>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/#transform-evaluate-on-downstream-tasks","title":"Transform &amp; Evaluate on Downstream Tasks","text":"<pre><code>cd experiments/llm-bitflip/transform\n\nmodel_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\" # HuggingFace model name\nx_p_exp=null                                    # bitflip probability for the sign-exp bits of the activation. Null means no bitflip.\nw_p_exp=null                                    # bitflip probability for the sign-exp bits of the weight. Null means no bitflip.\nx_zero_out_t=\"100\"                              # threshold for zeroing out outliers (and \"NaN\" values) of the activation\nw_zero_out_t=\"1.25\"                             # threshold for zeroing out outliers (and \"NaN\" values) of the weight\nx_p_frac=$(bc &lt;&lt;&lt; \"scale=10; 0.5^10\")           # bitflip probability for the mantissa bits of the activation\nw_p_frac=$(bc &lt;&lt;&lt; \"scale=10; 0.5^10\")           # bitflip probability for the mantissa bits of the weight\npython minimal.py eval-bitflip \\\n    --model_name ${model_name} \\\n    --bitflip_config \"default\" \\\n    --default_bitflip_config.x_p_exp=${x_p_exp} \\\n    --default_bitflip_config.x_p_frac=${x_p_frac} \\\n    --default_bitflip_config.x_zero_out_t=${x_zero_out_t} \\\n    --default_bitflip_config.w_p_exp=${w_p_exp} \\\n    --default_bitflip_config.w_p_frac=${w_p_frac} \\\n    --default_bitflip_config.w_zero_out_t=${w_zero_out_t} \\\n    --tasks ['wikitext']\n</code></pre> <p>eval-bitflip</p> <p>This <code>eval-bitflip</code> subcommand also uses <code>lm-eval-harness</code>'s <code>simple_evaluate</code> function. Please refer to the evaluation section of LLM Pretraining &amp; Evaluation for more details.</p>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/#evaluate-the-original-model","title":"Evaluate the Original Model","text":"<p>You may want to compare the evaluation results of the bitflip model with the original model. You can do this by running the following command:</p> <pre><code>python minimal.py eval-ori \\\n    --model_name ${model_name} \\\n    --tasks ['wikitext']\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/#test-the-generation","title":"Test the Generation","text":"<p>We also offer a <code>hf-gen</code> script to</p>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/#simple-generation","title":"Simple Generation","text":"<p>We provide a simple generation subcommand <code>hf-gen</code> as well.</p> <pre><code>prompt=\"London is\"\nmax_new_tokens=\"100\"\ndo_sample=\"true\"\ntemperature=\"0.6\"\ntop_k=\"50\"\ntop_p=\"0.9\"\n\npython minimal.py hf-gen \\\n    AICrossSim/clm-60m \\\n    ${prompt} \\\n    --max_new_tokens ${max_new_tokens} \\\n    --do_sample ${do_sample} \\\n    --temperature ${temperature} \\\n    --top_k ${top_k} \\\n    --top_p ${top_p} \\\n    --bitflip_config \"default\" \\\n    --default_bitflip_config.x_p_exp=${x_p_exp} \\\n    --default_bitflip_config.x_p_frac=${x_p_frac} \\\n    --default_bitflip_config.x_zero_out_t=${x_zero_out_t} \\\n    --default_bitflip_config.w_p_exp=${w_p_exp} \\\n    --default_bitflip_config.w_p_frac=${w_p_frac} \\\n    --default_bitflip_config.w_zero_out_t=${w_zero_out_t}\n</code></pre> <p>Our Initial Experiments</p> <p>For our <code>AICrossSim/clm-1.1b</code>, we sweep <code>x_p_frac</code> and <code>w_p_frac</code> and observe how perplexity and generated texts changes.</p> <p>Here are some samples of the generated texts: link</p> <p>Notably, when the perplexity is increase by 1%, the generated text are consistent with the original text.</p>"},{"location":"02-model-behaviour-level-simulation/clm-bitflip/#bitflip-aware-pretraining","title":"Bitflip-Aware Pretraining","text":"<p>Similar to the pretraining of script of AICrossSim-CLM (<code>experiments/llm-bitflip/pretrain/run.py</code>), we offer a <code>experiments/llm-bitflip/pretrain/run.py</code> script to run bitflip-aware pretraining from scratch. The subcommands accepts the same arguments as <code>experiments/llm-bitflip/pretrain/run.py</code>, but with an additional argument for bitflip transform.</p> <ul> <li> <p>For example, we can run the following command to run a bitflip-aware pretraining for <code>AICrossSim-CLM-60M</code> on 2 H100 96GB GPUs.</p> <ol> <li> <p>Generate a training config with bitflip transform config.</p> <pre><code>cd experiments/llm-bitflip/pretrain\n\nbitflip_transform_config=\"./configs/meta/fc-only-w-a-exp-frac.yaml\"\npython run.py generate-cfg \\\n    ${bitflip_transform_config} \\\n    --model_arch \"aixsim\" \\\n    --model_flavor \"60M\" \\\n    --batch_size 48 \\\n    --data_parallel_replicate_degree 2\\\n    --data_parallel_shard_degree -1 \\\n    --token_num_scale 22 \\\n    --compile \"false\" \\\n    --save_path \"./configs/tutorial-60m.yaml\"\n</code></pre> </li> <li> <p>Run the pretraining with the generated config.</p> <pre><code>num_gpus=\"2\"\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain \\\n    --config configs/tutorial-60m.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> </li> <li> <p>Convert the checkpoint to HuggingFace format.</p> <pre><code>torchrun_ckpt_path=\"path/to/torchrun/checkpoint\"\noutput_dir=\"path/to/output/dir\"\npython run.py convert-ckpt pt2hf \\\n    \"aixsim\" \"60M\" \\\n    ${torchrun_ckpt_path} \\\n    ${output_dir}\n</code></pre> </li> </ol> <p>Our Bitflip-Aware Training Results of AICrossSim-CLM-60M</p> <p>We performed bitflip-aware pretraining on <code>AICrossSim-CLM-60M</code> on 2 H100 96GB GPUs for 2.5 hours.</p> <ul> <li>Train config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-60M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/bitflip-fc-clm-60m</li> </ul> <ul> <li> <p>Similarly, one can run bitflip-aware pretraining for other AICrossSim-CLM model sizes. Here we summarize our current bitflip-aware pretraining progress.</p> Model Environment Pretraining Time Training Config Wandb Logs HuggingFace Checkpoint 60M 2x H100 96GB 2.5 hours <code>configs/aixsim-60M.yaml</code> link AICrossSim/bitflip-fc-clm-60m 200M 2x H100 96GB 14.3 hours <code>configs/aixsim-200M.yaml</code> link AICrossSim/bitflip-fc-clm-200m 400M 6x A6000 48GB 33 hours <code>configs/aixsim-400M.yaml</code> link AICrossSim/bitflip-fc-clm-400m 1.1B 8x H200 141GB 51 hours <code>configs/aixsim-1.1B.yaml</code> link AICrossSim/bitflip-fc-clm-1.1b </li> </ul> </li> </ul>"},{"location":"02-model-behaviour-level-simulation/clm-onn/","title":"Scaling Optical Transformers on Causal Language Models","text":"<p>After the Roberta-ONN experiments, we further scale the optical transformer experiments to causal language models (CLMs). In this tutorial, we demonstrate how to fine-tune a pre-trained CLM model using our optical transformer implementation with Mase-triton acceleration.</p>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#starting-point-of-training","title":"Starting Point of Training","text":"<p>We tried out three starting points for the ONN-CLM experiments and ended up using full fine-tuning of a pre-trained CLM as our main approach.</p> Starting Point Observations Codes Training an ONN CLM from scratch \ud83d\ude41 The training loss did not decrease link Parameter-efficient fine-tuning a pre-trained CLM using LoRA \ud83d\ude41 Only the training loss of 60M model decreased link Full fine-tuning a pre-trained CLM \u2705 The training loss decreases but requires a small learning rate &lt;1e-5 link"},{"location":"02-model-behaviour-level-simulation/clm-onn/#environment-setup","title":"Environment Setup","text":"<p>Environment Setup?</p> <p>If you have not set up environments, please follow the guidelines in Environment Setup.</p>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#full-fine-tuning-of-pre-trained-clm-with-optical-transformer","title":"Full Fine-tuning of Pre-trained CLM with Optical Transformer","text":"<p>Based on our experiments, full fine-tuning of pre-trained CLM models with optical transformer shows the most promising results. The optical transformer implementation uses Mase-triton acceleration to simulate optical computing operations with configurable quantization levels and smoothing factors.</p> <p>The entry point for optical transformer fine-tuning is at <code>experiments/llm-optical-transformer/continual_finetuning/run_clm_no_trainer.py</code>.</p>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#experiment-setup","title":"Experiment Setup","text":""},{"location":"02-model-behaviour-level-simulation/clm-onn/#optical-transformer-simulation-configuration","title":"Optical Transformer Simulation Configuration","text":"<p>The optical transformer configuration is controlled through a TOML file that specifies quantization parameters for both attention layers and fully-connected layers.</p> <p>The configuration file can be found at experiments/llm-optical-transformer/continual_finetuning/transform_cfg.toml, which contains the following key parameters:</p> <ul> <li><code>use_lora</code>: Set to <code>false</code> for full fine-tuning</li> <li><code>attention.q_levels</code>: Number of quantization levels (default: 256)</li> <li><code>attention.q_lut_min</code>: Minimum value for lookup table quantization (default: 0.020040)</li> <li><code>attention.q_smooth_factor</code>: Smoothing factor for quantization statistics (default: 0.9)</li> <li><code>attention.q_init_seed</code>: Random seed for initialization (default: 0)</li> <li><code>attention.q_bypass</code>: Whether to bypass quantization in attention layers (default: false)</li> <li><code>fc</code> has similar parameters for fully-connected layers</li> </ul>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#training-setup","title":"Training Setup","text":"Setting Description Pre-trained Model <code>AICrossSim/clm</code> series Dataset <code>Cheng98/fineweb-edu-1.25B</code>. We created a subset of CLM's pretraining dataset for convenience. Fine-tuning tokens 22 * N_params / 100 tokens Learning rate We sweep from 1e-7 to 1e-5 depending on model size. Larger models require smaller learning rates for stability. Effective batch size 16. Controlled through gradient accumulation steps and number of processes. <p>Info</p> <p>Detailed experiment configurations can be found in the provided Wandb logs.</p>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#fine-tuning-with-optical-transformer","title":"Fine-tuning with Optical Transformer","text":""},{"location":"02-model-behaviour-level-simulation/clm-onn/#basic-fine-tuning-command","title":"Basic Fine-tuning Command","text":"<p>The main script <code>run_clm_no_trainer.py</code> supports all standard Hugging Face training arguments plus optical transformer-specific configurations:</p> <pre><code>accelerate launch --num_processes=1 \\\n    run_clm_no_trainer.py \\\n    --model_name_or_path \"AICrossSim/clm-60m\" \\\n    --dataset_name \"Cheng98/fineweb-edu-1.25B\" \\\n    --per_device_train_batch_size 8 \\\n    --learning_rate 2e-5 \\\n    --weight_decay 0.01 \\\n    --num_train_epochs 1 \\\n    --gradient_accumulation_steps 2 \\\n    --lr_scheduler_type linear \\\n    --output_dir \"./output/clm-60m-optical\" \\\n    --preprocessing_num_workers 32 \\\n    --trust_remote_code \\\n    --with_tracking \\\n    --report_to wandb \\\n    --transform_cfg ./transform_cfg.toml \\\n    --block_size 1024 \\\n    --log_train_loss_steps 50\n</code></pre> <p>Learning Rate Selection</p> <p>Based on our experiments, optical transformer fine-tuning requires very small learning rates (&lt; 1e-5) to achieve stable training. Using larger learning rates may cause training instability. The larger the model, the smaller learning rate is recommended. A failed run may produce loss divergence like the following:</p> <p> CLM-400M with learning rate too high </p>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#using-bash-script","title":"Using Bash Script","text":"<p>For convenience, we provide a parameterized shell script <code>fine-tune-ot-clm.sh</code> that automatically calculates training steps and sets up appropriate configurations:</p> <pre><code># Basic usage with default parameters\n./fine-tune-ot-clm.sh\n\n# Customized parameters\n# Usage: ./fine-tune-ot-clm.sh [num_processes] [model_name_or_path] [per_device_train_batch_size] [learning_rate] [weight_decay] [gradient_accumulation_steps] [block_size]\n\n./fine-tune-ot-clm.sh 2 \"AICrossSim/clm-200m\" 4 \"1e-5\" 0.01 4 1024\n</code></pre> <p>The script automatically: 1. Calculates the appropriate number of training steps based on model size 2. Sets up output directories with descriptive names 3. Configures Wandb logging with relevant tags 4. Applies the optical transformer configuration</p>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#hyperparameter-sweeping","title":"Hyperparameter Sweeping","text":"<p>For learning rate exploration, use the <code>sweep.sh</code> script:</p> <pre><code># Edit sweep.sh to configure your desired learning rate ranges\n./sweep.sh\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/clm-onn/#results","title":"Results","text":"<p>Here we pick the training traces with smallest final training loss for each model size:</p> Optical Transformer Fine-tuning Results on CLM Models <p>The results indicate that full fine-tuning of pretrained optical CLM models does not scale as well as standard CLM fine-tuning. We only observe moderate improvements in training loss for smaller models (60M -&gt; 200M), while larger models (400M, 600M) even show degradation in performance.</p> <p>Info</p> <p>The traces above are smoothed for better visualization, and can be found in full detail in the Wandb logs.</p> 60M 200M 400M 600M Wandb Log Wandb Log Wandb Log Wandb Log <p>More traces with various learning rates can be found at Wandb Project: OT-CLM-full-ft.</p>"},{"location":"02-model-behaviour-level-simulation/dev-guidelines/","title":"Development Guidelines for Transform-Aware LLM Training","text":"<p>In this codebase, we can support both transform-ware continual pretraining and pretraining from scratch. However, if the transform is too lossy, the model may not be able to learn effectively if trained from scratch. continual pretraining is recommended in this case.</p>"},{"location":"02-model-behaviour-level-simulation/dev-guidelines/#continual-pretraining","title":"Continual Pretraining","text":"<p>Example: Continual Pretraining with Simulated Optical Compute</p> <p>The example scripts can be found at <code>experiments/llm-optical-transformer/continual_pretraining</code></p> <p>HuggingFace <code>transformers</code>'s Trainer is used to perform continual pretraining on the converted/pretrained checkpoint on HuggingFace. Our pretrained AICrossSim/clm checkpoints can be found in this collection</p> <p>Here we use optical compute in the Optical Transformers (<code>OT</code>) paper as an example. You may follow the following steps to implement other new compute paradigms. To implement <code>OT</code>, we have a few key components you can find in <code>src/aixsim_models/optical_compute/optical_transformer</code>:</p> <ol> <li> <p>Simulated OT linear layer and matmul</p> <ul> <li>class <code>OpticalTransformerLinear</code> to simulate the linear layer. All the linear layers in the pretrained model will be replace by this linear layer except for <code>lm_head</code>.</li> <li>function <code>OpticalTransformerFunctions.quantized_matmul_fn</code> to simulate the matmul. The matmul is wrapped in <code>HFOpticalTransformerLlamaAttention</code> to simulate the Query-Key matmul and Attention-Value matmul.</li> </ul> <p>Kernels in <code>mase_triton.optical_compute</code></p> <ul> <li> <p>We use Triton instead of PyTorch API to implement <code>OpticalTransformerLinear</code> (essentially functional <code>ot_qlinear_fn</code>) and <code>OpticalTransformerFunctions.quantized_matmul_fn</code>, because for the method described in Optical Transformers, if we implement it using PyTorch built-in functions, the training will consume a lot of GPU memory and the training speed will be very slow. We implement Triton kernel mainly for saving GPU memory. If your simulation can be memory-effciently implemented using PyTorch built-in functions, you don't need to use Triton.</p> </li> <li> <p>HuggingFace <code>transformers</code>'s Trainer may not work with autotuned Triton kernels. This is why in <code>mase-triton</code>, the autotuning is disabled.</p> </li> </ul> </li> <li> <p>A pass to transform <code>LlamaForCausalLM</code>.</p> <p>We implement the function <code>transform_hf_model</code> to transform the model. Inside the function, there are two for loops, one for replacing attention layer with <code>HFOpticalTransformerLlamaAttention</code> to replace matmuls and the other for replacing linear layer with <code>OpticalTransformerLinear</code>.</p> </li> <li> <p>Transform config</p> <p>We use a YAML file to specify the transform config (<code>configs/default.yaml</code>). In <code>transform_hf_model</code>'s for loop, the <code>TransformConfigManager</code> uses the layer name to find the corresponding transform config.</p> </li> </ol> <p>With these two components, we can simply adapt HuggingFace's <code>run_clm.py</code> such that the model is transformed before training starts. The adapted <code>run_clm.py</code> can be found here.</p> <ul> <li> <p>In the adapted <code>run_clm.py</code>, we insert the following code snippet to transform the model before training starts:</p> <pre><code>    if model_args.transform_config is not None:\n        with open(model_args.transform_config, \"r\") as f:\n            transform_args = yaml.safe_load(f)\n        config_manager = TransformConfigManager(**transform_args)\n        transformed_layers = transform_hf_model(model, config_manager)\n        transform_histogram = make_transform_histogram(transformed_layers=transformed_layers)\n        logger.info(f\"\ud83d\udd0d Transformed layers:\\n{transform_histogram}\")\n    else:\n        logger.info(\"\u26a0\ufe0f No transform config file provided. Using the original model.\")\n</code></pre> </li> <li> <p>You may copy the adapted <code>run_clm.py</code> and replace the <code>OT</code> transform pass <code>transform_hf_model</code> with your own transform pass.</p> </li> </ul> <p>Then as shown in the <code>justfile</code>, we can launch the optical compute aware continual pretraining by:</p> <pre><code># This run uses small batch size and training steps for demonstration purpose.\npython run_clm.py \\\n    --model_name_or_path AICrossSim/clm-60m \\\n    --dataset_name HuggingFaceFW/fineweb-edu \\\n    --dataset_config_name \"sample-10BT\" \\\n    --per_device_train_batch_size 12 \\\n    --per_device_eval_batch_size 12 \\\n    --gradient_accumulation_steps 50 \\\n    --do_train \\\n    --report_to \"wandb\" \\\n    --learning_rate 5e-5 \\\n    --max_steps 100 \\\n    --save_strategy \"steps\" \\\n    --save_steps 500 \\\n    --save_total_limit 2 \\\n    --bf16 \\\n    --dataloader_num_workers 16 \\\n    --preprocessing_num_workers 32 \\\n    --tokenizer_name HuggingFaceTB/cosmo2-tokenizer \\\n    --output_dir ./output/test-clm-trainer \\\n    --transform_config ./configs/default.yaml \\\n    --seed 42\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/dev-guidelines/#pretraining-from-scratch","title":"Pretraining from Scratch","text":"<p>We use torchtitan as the backend to pretrain transformed LLM from scratch. Please refer to <code>experiments/llm-optical-transformer/pretrain/run.py</code>.</p>"},{"location":"02-model-behaviour-level-simulation/mase-triton/","title":"MASE-Triton","text":"<p>PyPI Link | GitHub Link</p> <p>Mase-triton is a PyTorch extension library that provides efficient implementations of various operations used in simulating new compute paradigms and our PLENA project, including random bitflip, optical transformer, MXFP (Microscaling Formats), and minifloat. It leverages the Triton language to enable faster simulations on CUDA-enabled GPUs.</p>"},{"location":"02-model-behaviour-level-simulation/mase-triton/#functionality","title":"Functionality","text":"<ul> <li> <p>Random Bitflip: Simulate random bit flips in neural network computations</p> <ul> <li><code>functional APIs</code>: Random bitflip functions with backward support.<ul> <li><code>random_bitflip_fn</code>: Perform random bit flipping on tensors with configurable exponent and fraction bit flip probabilities</li> <li><code>calculate_flip_probability</code>: Calculate flip probability from number of halves</li> <li><code>find_nearest_prob_n_halves</code>: Find nearest probability in terms of halves</li> </ul> </li> <li><code>layers</code>: PyTorch modules for random bitflip operations.<ul> <li><code>RandomBitFlipDropout</code>: Random bit flip layer with dropout functionality</li> <li><code>RandomBitFlipLinear</code>: Linear layer with random bit flipping</li> </ul> </li> </ul> </li> <li> <p>Optical Transformer: Simulate optical computing for transformers</p> <ul> <li><code>functional APIs</code>: Optical transformer functions with backward support.<ul> <li><code>ot_quantize_fn</code>: Quantize tensors for optical transformer operations</li> <li><code>ot_qlinear_fn</code>: Quantized linear transformation for optical computing</li> <li><code>ot_qmatmul_fn</code>: Quantized matrix multiplication for optical computing</li> </ul> </li> <li><code>layers</code>: PyTorch modules for optical computing.<ul> <li><code>OpticalTransformerLinear</code>: Linear layer with optical transformer quantization</li> </ul> </li> </ul> </li> <li> <p>MXFP: Simulate MXFP (Microscaling Formats) on CPU &amp; GPU using PyTorch &amp; Triton</p> <ul> <li><code>functional</code>: MXFP format conversion and operations.<ul> <li><code>extract_mxfp_components</code>: Extract MXFP components (shared exponent and elements) from tensors</li> <li><code>compose_mxfp_tensor</code>: Compose MXFP components back to standard floating-point tensors</li> <li><code>quantize_dequantize</code>: Quantize and dequantize tensors using MXFP format</li> <li><code>flatten_for_quantize</code>: Flatten tensors for quantization operations</li> <li><code>permute_for_dequantize</code>: Permute tensors for dequantization operations</li> <li><code>mxfp_linear</code>: Linear operation with MXFP support</li> <li><code>mxfp_matmul</code>: Matrix multiplication with MXFP support</li> <li><code>parse_mxfp_linear_type</code>: Parse MXFP linear layer types</li> </ul> </li> <li><code>layers</code>: PyTorch modules with MXFP support.<ul> <li><code>MXFPLinearPTQ</code>: Linear layer with MXFP support for post-training quantization (no backpropagation support)</li> </ul> </li> </ul> </li> <li> <p>Minifloat: Simulate minifloat formats on CPU &amp; GPU using PyTorch &amp; Triton</p> <ul> <li><code>functional</code>: Minifloat format operations.<ul> <li><code>extract_minifloat_component</code>: Extract minifloat components from tensors</li> <li><code>compose_minifloat_component</code>: Compose minifloat components back to tensors</li> <li><code>quantize_dequantize</code>: Quantize and dequantize tensors using minifloat format</li> <li><code>minifloat_linear</code>: Linear operation with minifloat support</li> <li><code>minifloat_matmul</code>: Matrix multiplication with minifloat support</li> </ul> </li> <li><code>layers</code>: PyTorch modules with minifloat support.<ul> <li><code>MinifloatLinearPTQ</code>: Linear layer with minifloat support for post-training quantization (no backpropagation support)</li> </ul> </li> </ul> </li> <li> <p>Utilities &amp; Management</p> <ul> <li><code>manager.py</code>: Kernel management and autotune control.<ul> <li><code>KernelManager</code>: Enable/disable autotune for Triton kernels</li> </ul> </li> <li><code>utils/</code>: Various utility functions for PyTorch modules, debugging, and training.</li> </ul> </li> </ul>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/","title":"RoBERTa Optical Transformer","text":"<p>This tutorial demonstrates how to apply optical transformer modifications to RoBERTa models for sequence classification tasks. The optical transformer implementation simulates photonic computing operations with quantization-aware attention mechanisms and linear layers.</p>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#overview","title":"Overview","text":"<ul> <li>Optical Transform: Applies optical computing simulation to RoBERTa models by replacing standard attention and linear layers with optical transformer equivalents.<ul> <li>The entry point is at <code>experiments/roberta-optical-transformer/run_glue.py</code>.</li> </ul> </li> <li>Optical Attention: Custom attention mechanism with quantization-aware operations simulating optical matrix operations.<ul> <li>Implemented in <code>src/aixsim_models/optical_compute/optical_transformer/fine_tune/ot_roberta.py</code>.</li> </ul> </li> <li>GLUE Task Support: Fine-tune and evaluate optical RoBERTa models on GLUE benchmark tasks.<ul> <li>Configuration scripts available at <code>experiments/roberta-optical-transformer/finetune_base.sh</code>.</li> </ul> </li> </ul> <p>The optical transformer simulation uses custom triton kernels from <code>mase-triton</code> to accelerate quantization-aware operations: - Optical Attention: Implements quantized matrix operations for Q, K, V projections with configurable quantization levels. - Optical Linear: Replaces standard linear layers with quantization-aware optical equivalents. - Quantization Parameters: Supports configurable quantization levels, smoothing factors, noise injection, and bypass modes.</p>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#environment-setup","title":"Environment Setup","text":"<p>Environment Setup?</p> <p>If you have not set up environments, please follow the guidelines in Environment Setup.</p>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#optical-transform-configuration","title":"Optical Transform Configuration","text":"<p>The optical transformer behavior is controlled through a YAML configuration file that specifies quantization parameters for both attention (<code>attn</code>) and fully connected (<code>fc</code>) layers.</p>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#configuration-parameters","title":"Configuration Parameters","text":"<p>The transform configuration includes the following parameters:</p> <ul> <li><code>q_levels</code>: Number of quantization levels (default: 256)</li> <li><code>q_lut_min</code>: Minimum lookup table value for quantization (default: 0.020040)</li> <li><code>q_quantiles</code>: Optional quantile-based range setting (default: null)</li> <li><code>q_smooth_factor</code>: Smoothing factor for statistics updates (default: 0.9)</li> <li><code>q_init_seed</code>: Random seed for initialization (default: 0)</li> <li><code>q_bypass</code>: Whether to bypass optical transform (default: false)</li> </ul>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#default-configuration","title":"Default Configuration","text":"<pre><code># experiments/roberta-optical-transformer/transform_cfg.yaml\n\"attn\":\n  q_levels: 256\n  q_lut_min: 0.020040\n  q_quantiles: null\n  q_smooth_factor: 0.9\n  q_init_seed: 0\n  q_bypass: false\n\"fc\":\n  q_levels: 256\n  q_lut_min: 0.020040\n  q_quantiles: null\n  q_smooth_factor: 0.9\n  q_init_seed: 0\n  q_bypass: false\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#fine-tuning-roberta-with-optical-transform","title":"Fine-tuning RoBERTa with Optical Transform","text":""},{"location":"02-model-behaviour-level-simulation/roberta-onn/#single-task-fine-tuning","title":"Single Task Fine-tuning","text":"<p>Fine-tune an optical RoBERTa model on a specific GLUE task:</p> <pre><code>cd experiments/roberta-optical-transformer\n\n# Set task parameters\nTASK_NAME=\"mrpc\"                                    # GLUE task (mrpc, sst2, cola, etc.)\nMODEL_NAME=\"FacebookAI/roberta-base\"                # Base model\nLEARNING_RATE=\"2e-5\"                               # Learning rate\nBATCH_SIZE=\"16\"                                    # Batch size\nNUM_EPOCHS=\"3\"                                     # Training epochs\nTRANSFORM_CONFIG=\"transform_cfg.yaml\"              # Optical transform config\n\npython run_glue.py \\\n    --model_name_or_path ${MODEL_NAME} \\\n    --task_name ${TASK_NAME} \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size ${BATCH_SIZE} \\\n    --learning_rate ${LEARNING_RATE} \\\n    --num_train_epochs ${NUM_EPOCHS} \\\n    --output_dir ./output/${TASK_NAME}_optical \\\n    --overwrite_output_dir \\\n    --transform_config ${TRANSFORM_CONFIG} \\\n    --evaluation_strategy epoch \\\n    --save_strategy epoch \\\n    --logging_steps 50 \\\n    --seed 42\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#multi-task-fine-tuning","title":"Multi-Task Fine-tuning","text":"<p>Fine-tune on multiple GLUE tasks using the provided shell script:</p> <pre><code>cd experiments/roberta-optical-transformer\n\n# Configure multi-task parameters in finetune_base.sh\nexport USE_SINGLE_TASK=false\nexport TASK_LIST=\"stsb mrpc cola\"\nexport LR_LIST=\"1e-3 2e-5 1e-5\"\nexport MODEL_NAME=\"FacebookAI/roberta-base\"\nexport BATCH_SIZE=16\n\n# Run multi-task fine-tuning\nbash finetune_base.sh\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#evaluation-only","title":"Evaluation Only","text":"<p>Evaluate a pre-trained optical RoBERTa model without training:</p> <pre><code>python run_glue.py \\\n    --model_name_or_path ${MODEL_NAME} \\\n    --task_name ${TASK_NAME} \\\n    --do_eval \\\n    --max_seq_length 128 \\\n    --per_device_eval_batch_size ${BATCH_SIZE} \\\n    --output_dir ./output/${TASK_NAME}_eval \\\n    --transform_config ${TRANSFORM_CONFIG} \\\n    --overwrite_output_dir\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#baseline-comparison","title":"Baseline Comparison","text":"<p>To compare optical transformer performance with the original model, run without the transform configuration:</p> <pre><code>python run_glue.py \\\n    --model_name_or_path ${MODEL_NAME} \\\n    --task_name ${TASK_NAME} \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size ${BATCH_SIZE} \\\n    --learning_rate ${LEARNING_RATE} \\\n    --num_train_epochs ${NUM_EPOCHS} \\\n    --output_dir ./output/${TASK_NAME}_baseline \\\n    --overwrite_output_dir \\\n    --evaluation_strategy epoch \\\n    --save_strategy epoch\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/roberta-onn/#results","title":"Results","text":"<p>Post-training transform results (Applying optical transform to a trained RoBERTa model):</p> Model MNLI (Acc/mismatch) QNLI (Acc) RTE (Acc) SST (Acc) MRPC (Acc) CoLA (Matt) QQP (Acc) STSB (P/S corr) Avg (Avg) Original 0.8728 0.9244 0.7978 0.9357 0.9019 0.6232 0.9153 0.9089 0.8600 Random 0.3266 0.4946 0.5271 0.4908 0.3162 0.0000 0.6318 0.0332 0.3525 Optical Transformer 0.8000 0.7966 0.4801 0.8704 0.7770 0.2034 0.9075 0.8485 0.7104 SqueezeLight 0.3200 0.4961 0.4404 0.5126 0.5025 0.0213 0.5890 -0.0543 0.3582 <p>Fine-tuning results (Transform-aware fine-tuning on a trained RoBERTa model):</p> Model MNLI (Acc/mismatch) QNLI (Acc) RTE (Acc) SST (Acc) MRPC (Acc) CoLA (Matt) QQP (Acc) STSB (P/S corr) Avg (Avg) Original 0.8728 0.9244 0.7978 0.9357 0.9019 0.6232 0.9153 0.9089 0.8600 Random 0.3266 0.4946 0.5271 0.4908 0.3162 0.0 0.6318 0.0332 0.3525 Optical Transformer 0.8510 0.9032 0.5813 0.9140 0.8677 0.4441 0.9060 0.0332 0.6876 SqueezeLight 0.3212 0.4961 0.4676 0.5131 0.5025 0.0 0.5932 0.0514 0.3681 <p>Takeaways: - Whether post-training transform or transform-aware fine-tuning, the optical transformer significantly outperforms SqueezeLight. This is mainly because SqueezeLight was designed for convolutional networks. - The continual fine-tuning of the optical transformer usually yields better performance than post-training transform, but sometimes the noisy forward pass and straight-through estimator in the backward pass can break the training stability, leading to suboptimal results like STSB. - We decide to keep the optical transformer only for future large-scale experiments.</p>"},{"location":"02-model-behaviour-level-simulation/vit-cim/","title":"Vision Transformer CIM Simulation","text":"<p>This tutorial demonstrates how to apply Compute-in-Memory (CIM) transformations to Vision Transformer (ViT) models for CIM-aware fine-tuning.</p>"},{"location":"02-model-behaviour-level-simulation/vit-cim/#overview","title":"Overview","text":"<ul> <li>CIM-aware fine-tuning takes a pretrained ViT model, applies CIM transformation, and fine-tunes the model on downstream vision datasets.<ul> <li>The entry point is at <code>experiments/vit-cim/run_vit.py</code>.</li> </ul> </li> </ul> <p>The CIM transformation simulates the effect of compute-in-memory architectures, with both digital and analog support.</p>"},{"location":"02-model-behaviour-level-simulation/vit-cim/#evaluation-of-cim-aware-fine-tuning","title":"Evaluation of CIM-aware Fine-tuning","text":"<p>Environment Setup</p> <p>If you have not set up environments, please follow the guidelines in Environment Setup.</p> <p>We provide scripts to apply CIM-aware fine-tuning on Vision Transformer models and evaluate their performance on standard vision benchmarks.</p>"},{"location":"02-model-behaviour-level-simulation/vit-cim/#cim-aware-fine-tuning-evaluate-on-vision-tasks","title":"CIM-aware Fine-tuning &amp; Evaluate on Vision Tasks","text":"<pre><code>git clone https://github.com/AICrossSim/NewComputeBench.git\ncd NewComputeBench\n\nmodel_name=\"google/vit-base-patch16-224\"    # HuggingFace ViT model\ndataset_name=\"imagenet\"                      # Vision dataset for evaluation\ncim_config_path=\"./experiments/llm-cim/configs/sram.yaml\" # CIM transformation configuration\noutput_dir=\"./log_eval_results\"             # Output directory for results\n\npython experiments/vit-cim/run_vit.py \\\n    --model_name_or_path ${model_name} \\\n    --dataset_name ${dataset_name} \\\n    --cim_config_path ${cim_config_path} \\\n    --output_dir ${output_dir} \\\n    --per_device_eval_batch_size 32 \\\n    --enable_cim_transform \\\n    --do_eval\n</code></pre> <p>CIM Configuration</p> <p>The CIM configuration file defines the noise characteristics, quantization levels, and other parameters that simulate the analog compute-in-memory effects. See <code>experiments/llm-cim/configs/</code> for example configurations.</p>"},{"location":"02-model-behaviour-level-simulation/vit-cim/#cim-configuration-examples","title":"CIM Configuration Examples","text":""},{"location":"02-model-behaviour-level-simulation/vit-cim/#typical-sram-cim-configuration","title":"Typical SRAM CIM Configuration","text":"<pre><code># experiments/llm-cim/configs/sram.yaml\nby: \"type\"\nconv2d:\n  config:\n    tile_type: \"digital\"\n    core_size: 16\n    rescale_dim: \"vector\"\n    x_quant_type: \"e4m3\"\n    weight_quant_type: \"e4m3\"\n\nlinear:\n  config:\n    tile_type: \"digital\"\n    core_size: 64\n    rescale_dim: \"vector\"\n    x_quant_type: \"e4m3\"\n    weight_quant_type: \"e4m3\"\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/vit-cim/#supported-models-and-datasets","title":"Supported Models and Datasets","text":""},{"location":"02-model-behaviour-level-simulation/vit-cim/#models","title":"Models","text":"<p>Support ViT model family from huggingface: e.g.<code>google/vit-base-patch16-224</code></p>"},{"location":"02-model-behaviour-level-simulation/vit-cim/#datasets","title":"Datasets","text":"<ul> <li>ImageNet: 1000-class image classification (requires custom path)</li> </ul>"},{"location":"02-model-behaviour-level-simulation/vit-cim/#performance-metrics","title":"Performance Metrics","text":"<p>The evaluation provides comprehensive metrics:</p> <ul> <li>Accuracy: Top-1 and Top-5 accuracy</li> </ul>"}]}