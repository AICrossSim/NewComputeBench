{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AICrossSim/NewComputeBench","text":"NewComputeBench <p>AICrossSim/NewComputeBench is a benchmark suite for new compute paradigms (Spiking neural networks, Optical computation, In-Memory computation, etc) via software emulation. We aim to predict the scaling law of neural networks trained with new compute paradigms by running small &amp; medium scale experiments and extrapolate the trends we observed. NewComputeBench project mainly consists of three parts:</p> <ul> <li>Model Training</li> <li>Model Behavior-Level Simulation</li> <li>Hardware-Performance Simulation (<code>\ud83d\udea7 TODO</code>)</li> </ul>"},{"location":"#whats-new","title":"What's New","text":"<ul> <li> <p>\ud83d\udea9 15th April Milestone: System and model-level training simulation (Small Language Models).</p> Item Description Environment setup Tutorial Pretraining AICrossSim LLMs (60M, 200M, 400M, 1.1B) &amp; evaluation Tutorial Software-emulated bitflip-aware pretraining &amp; evaluation Tutorial </li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Model Training &amp; Evaluation<ul> <li>LLMs<ul> <li> Pretraining of LLMs (60M, 200M, 400M, 1.1B) using the Llama-3 architecture.</li> <li> <code>lm-eval-harness</code> evaluation of LLMs.</li> <li> Parameter-efficient fine-tuning</li> <li> Supervised fine-tuning</li> </ul> </li> </ul> </li> <li>Model Behavior-Level Simulation<ul> <li> Post-training bitflip transform &amp; bitflip-aware pretraining</li> <li> Optical compute</li> <li> Spiking neural networks</li> <li> In-memory compute</li> </ul> </li> <li>Hardware-Performance Simulation<ul> <li> Hardware performance prediction</li> <li><code>\ud83d\udea7 TODO</code></li> </ul> </li> </ul>"},{"location":"#about-the-project","title":"About the Project","text":"<p>This project is led by Dr. Yiren Zhao at Imperial College London, Dr. Luo Mai at University of Edinburgh, Prof. Robert Mullins at University of Cambridge, and funded by Advanced Research + Invention Agency (ARIA).</p>"},{"location":"dev-guide/","title":"Developer Guide","text":""},{"location":"dev-guide/#environment-setup","title":"Environment Setup","text":"<p>Besides the environment setup in Environment Setup, you need to install <code>mkdocs-material</code> for maintaining the documentation.</p> <pre><code>pip install mkdocs-material mkdocs-git-revision-date-localized-plugin mkdocs-git-committers-plugin-2\n</code></pre>"},{"location":"dev-guide/#documentation","title":"Documentation","text":"<p>Currently we maintain the deliverable documentation in <code>docs</code> folder. The documentation is built using MkDocs and Material for MkDocs, and each markdown file is a page in the documentation. Everytime a new commit is pushed to the <code>main</code> branch, the documentation will be automatically built and deployed to GitHub Pages.</p>"},{"location":"dev-guide/#how-to-add-a-new-page","title":"How to add a new page?","text":"<ol> <li>Create a new markdown file in the <code>docs</code> folder.</li> <li>Add the new page to the <code>mkdocs.yml</code> file under the <code>nav</code> section.</li> </ol>"},{"location":"dev-guide/#how-to-preview-the-documentation","title":"How to preview the documentation?","text":"<p>Run the following command in the root directory of the project:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then you can preview the static sites in your browser.</p>"},{"location":"env-setup/","title":"Environment Setup","text":""},{"location":"env-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux or WSL2</li> <li>CUDA-enabled GPU</li> <li>MiniConda or Anaconda (for installing Cuda-Toolkit)</li> </ul> <p>Our Environment Setup for Reference</p> <p>We run all the experiments on linux machines and did not test on Windows.</p> <p>Here are a few environment we have tested for reference:</p> <ul> <li>NVIDIA A6000 48GBx8, Ubuntu 24.04, CUDA 12.4</li> <li>NVIDIA H100 96GBx2, Red Hat Enterprise Linux 9.5, CUDA 12.6.</li> <li>NVIDIA H100 80GBx8, Ubuntu 24.04, CUDA 12.4</li> <li>NVIDIA H200 141GBx8, Ubuntu 24.04, CUDA 12.4</li> </ul>"},{"location":"env-setup/#environment-setup_1","title":"Environment Setup","text":"<ol> <li> <p>Config SSH key for GitHub. One of the dependencies, MASE, requires SSH to clone and install. Please set up <code>~/.ssh/config</code> accordingly (refer to Connecting to GitHub with SSH).</p> </li> <li> <p>Clone the project repository</p> <pre><code>git clone https://github.com/AICrossSim/NewComputeBench.git\ncd NewComputeBench\ngit submodule update --init\n</code></pre> </li> <li> <p>Create a new conda environment</p> <pre><code>conda env create -f environment.yaml\n</code></pre> </li> <li> <p>Activate the new environment and install required packages</p> <pre><code>conda activate new-compute\n</code></pre> <p>We recommend check if the python and pip in <code>$PATH</code> are from the conda environment: <pre><code>which python\nwhich pip\n</code></pre></p> <p>Then install the required packages: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>(Optional) You may want to log in Wandb to track the training logs.</p> <pre><code>wandb login\n</code></pre> </li> </ol>"},{"location":"model-list/","title":"Models","text":"<p>A list of models we aim to port to NewComputeBench.</p> Task Type Model Name Model Sizes Description Causal language modeling <code>AICrossSim-CLM</code> 60M, 200M, 400M, 1.1B A family of small language models using Llama-3.1 architecture.  We use <code>cosmo2-tokenizer</code> and pretrain them on Fineweb-Edu. Causal language modeling <code>Llama-3</code> 1B, 3B, 8B, 70B Meta's Llama-3 model family Causal language modeling TBD TBD TBD Image generation TBD TBD TBD Image classification TBD TBD TBD"},{"location":"model-list/#model-training","title":"Model Training","text":"<ul> <li> <p>Pretraining from scratch</p> Model Names Supported? <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 </li> <li> <p>Evaluation</p> Task Model Name Supported? Causal language modeling <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 </li> <li> <p><code>\ud83d\udea7 TODO</code> Fine-tuning</p> </li> </ul>"},{"location":"model-list/#model-behavior-level-simulation","title":"Model Behavior-Level Simulation","text":"<ul> <li> <p>Transform-aware pretraining from scratch</p> Transform Model Name Supported? Random Bitflip <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u2705 Optical Compute <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f In-Memory Compute <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f Spiking Neural Networks <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f </li> <li> <p>Post-transform/training evaluation</p> Transform Task Model Name Supported? Random Bitflip Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f Optical Compute Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f In-Memory Compute Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f Spiking Neural Networks Benchmarks in lm-eval-harness <code>AICrossSim-CLM</code>, <code>Llama-3</code> \u23f9\ufe0f </li> </ul>"},{"location":"model-list/#hardware-performance-simulation","title":"Hardware-Performance Simulation","text":"<p><code>\ud83d\udea7 TODO</code></p>"},{"location":"01-model-training/llm-pretrain-and-eval/","title":"LLM Pretraining","text":"<p>This is a tutorial on how to pretrain AICrossSim-CLM using NewComputeBench.</p>"},{"location":"01-model-training/llm-pretrain-and-eval/#overview","title":"Overview","text":"<ul> <li>We aim to pretrain AICrossSim-CLM models (60M, 200M, 400M, 1.1B) on the Fineweb-Edu dataset.<ul> <li>We followed the Chinchilla scaling law to determine the number of training tokens: <code>num_tokens = 22 * num_params</code>.</li> <li>As the model size increases, the training time and memory requirements will increase significantly. For example, we pretrained the 1.1B model on 8 NVIDIA H100 80GB GPUs for 1.4 days, while the 60M model can be pretrained on 2 NVIDIA H100 80GB GPUs within 1 hour.</li> </ul> </li> <li>The pretraining entrypoint is at <code>experiments/llm-digital/pretrain/run.py</code><ul> <li><code>run.py</code> supports multiple subcommands, including <code>pretrain</code>, <code>eval</code>, <code>generate-hf</code>, <code>convert-ckpt</code>, and <code>generate-cfg</code>.<ul> <li>Run <code>python run.py -h</code> to see the available subcommands.</li> <li>Run <code>python run.py &lt;subcommand&gt; -h</code> to see the help message for a specific subcommand.</li> </ul> </li> <li>To run distributed training, we use <code>torchrun</code> to launch the training script.</li> </ul> </li> <li>We uploaded the pretrained models to HuggingFace for easy access: NewComputeBench-CLM-Digital</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#pretraining","title":"Pretraining","text":"<p>Environment Setup?</p> <p>If you have not set up environments, please follow the guidelines in Environment Setup.</p>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-60m","title":"AICrossSim-CLM-60M","text":"<p>We demonstrate the pretraining process using the <code>AICrossSim-CLM-60M</code> model. The same process can be applied to other models with minor adjustments.</p> <ol> <li> <p>Change the working directory to <code>experiments/llm-digital/pretrain</code> and activate the conda environment.</p> <pre><code>cd experiments/llm-digital/pretrain\nconda activate new-compute\n</code></pre> </li> <li> <p>Generate pretraining config</p> <p>Fast Development Run?</p> <p><code>generate-cfg</code> has several default arguments. You may want to change them for a fast development run:</p> <ul> <li><code>--batch_size</code>: a smaller batch size to avoid out-of-memory errors.</li> <li><code>--data_parallel_replicate_degree</code>: partition the training data across multiple GPUs. Each GPU receives a subset of the training data.</li> <li><code>--data_parallel_shard_degree</code>: partition the model parameters across multiple GPUs. Each GPU receives a subset of the model parameters. Default <code>-1</code> means no sharding.</li> <li><code>--token_num_scale</code>: the scale used to determine the number of training tokens: <code>num_tokens = token_num_scale * num_params</code>, 22 by default. Set this to a small value like <code>1</code> to reduce the number of training steps.</li> </ul> <pre><code>data_parallel=\"2\"       # For Simplicity, we set this to number of GPUs per node\nbatch_size=\"48\"         # Per-device batch size\ntoken_num_scale=\"22\"    # Scale for number of training tokens\npython run.py generate-cfg \\\n    --model_flavor 60M --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel} \\\n    --compile true \\\n    --save_path ./configs/tutorial-60M.yaml\n</code></pre> <ul> <li>This will generate a training configuration file <code>configs/tutorial-60M.yaml</code> for pretraining <code>AICrossSim-CLM-60M</code> model using a per-device batch size of 48 and a data parallel replicate degree of 2 on a FineWeb-Edu subset of <code>22 * 60M</code> tokens.</li> <li>Subcommand <code>generate-cfg</code> automatically calculates the number of training steps.</li> <li>The <code>--compile</code> flag enables the use of <code>torch.compile</code> for optimizing the training process.</li> </ul> </li> <li> <p>Launch pretraining</p> <pre><code>num_gpus=\"2\" # Number of GPUs per node. We only use one node for this example\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-60M.yaml \\\n    --metrics_args.enable_wandb false # disable wandb in case the user does not log in wandb\n</code></pre> <ul> <li>This will pass the generated configuration file and launch the pretraining job on a single node of 2 GPUs using <code>torchrun</code>.</li> <li>The <code>--metrics_args.enable_wandb</code> flag disables Weights and Biases logging. You can enable it by setting it to <code>true</code>.</li> <li>The <code>STREAM_HF_DATA</code> environment variable is set to <code>1</code> to enable streaming data loading from Hugging Face datasets instead of downloading the huge dataset to the local disk.</li> <li>When the training is finished, the model checkpoint will be saved at <code>./outputs/checkpoints/aixsim-60M/&lt;timestamp&gt;</code>.</li> </ul> Fatal Python error: Aborted ? <p>We noticed that after the training is finished, <code>torchrun</code> may raise the error \"Fatal Python error: Aborted\" when destroying process group. This does not affect the training results as long as the error is raised after the final checkpoint is saved (messages like \"[rank0]:2025-04-01 00:25:59,616 - root - INFO - Finished saving the checkpoint (or staging if async is enabled)in 5.53 seconds.\")</p> <p>Here is an example of the error message:</p> <pre><code>[rank0]:2025-04-01 00:25:47,084 - root - INFO - step: 640/644 = 99.3789%  loss:  6.2116  memory: 87.04GiB(93.48%)  tps: 52,142  mfu: 3.09%\n[rank0]:2025-04-01 00:25:54,090 - root - INFO - Saving a full checkpoint at last step, step 644.\n[rank0]:2025-04-01 00:25:59,616 - root - INFO - Finished saving the checkpoint (or staging if async is enabled)in 5.53 seconds.\n[rank0]:2025-04-01 00:25:59,617 - root - INFO - Sleeping 2 seconds for other ranks to complete\n[rank0]:2025-04-01 00:26:01,706 - root - INFO - Training completed\n[rank0]:terminate called without an active exception\n[rank0]:Fatal Python error: Aborted\n[rank0]:\n[rank0]:Current thread 0x00007f0360ff9640 (most recent call first):\n[rank0]:  Garbage-collecting\n[rank0]:  &lt;no Python frame&gt;\n[rank0]:\n[rank0]:Thread 0x00007f06b6639200 (most recent call first):\n[rank0]:  &lt;no Python frame&gt;\nW0401 00:26:06.901000 3016633 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3016738 closing signal SIGTERM\nE0401 00:26:08.212000 3016633 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 1 (pid: 3016739) of binary: /home/zz7522/miniconda3/envs/new-compute/bin/python3.11\nTraceback (most recent call last):\nFile \"/home/zz7522/miniconda3/envs/new-compute/bin/torchrun\", line 8, in &lt;module&gt;\n    sys.exit(main())\n            ^^^^^^\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n        ^^^^^^^^^^^^^^^^^^\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/run.py\", line 918, in main\n    run(args)\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/run.py\", line 909, in run\n    elastic_launch(\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zz7522/miniconda3/envs/new-compute/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n========================================================\nrun.py FAILED\n--------------------------------------------------------\nFailures:\n&lt;NO_OTHER_FAILURES&gt;\n--------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\ntime      : 2025-04-01_00:26:06\nhost      : ee-tiamat.ee.ic.ac.uk\nrank      : 1 (local_rank: 1)\nexitcode  : -6 (pid: 3016739)\nerror_file: &lt;N/A&gt;\ntraceback : Signal 6 (SIGABRT) received by PID 3016739\n========================================================\n</code></pre> </li> <li> <p>(Optional) Convert to HuggingFace checkpoint</p> <p>HuggingFace Checkpoint</p> <p>To support distributed training, the training code defines custom model classes, and the checkpoints are saved in a custom format by <code>torchrun</code>. To exploit the HuggingFace ecosystem, we provide a script to convert the custom checkpoint to HuggingFace format.</p> <pre><code>python run.py convert-ckpt aixsim 60M \\\n    ./outputs/checkpoints/aixsim-60M/&lt;timestamp&gt;/&lt;step-xxx&gt; \\\n    path/to/huggingface/checkpoint\n</code></pre> </li> </ol> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-60M</code> model on 2 NVIDIA H100 96GB GPUs for 1 hour.</p> <ul> <li>Training config: <code>experiments/llm-digital/pretrain/configs/aixsim-60M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-60m</li> </ul> <p>Similarly, you can pretrain the other models by changing the <code>--model_flavor</code> argument to <code>200M</code>, <code>400M</code>, or <code>1.1B</code>, and adjusting <code>--batch_size</code>, <code>--data_parallel_replicate_degree</code>, <code>--data_parallel_shard_degree</code>, and <code>--token_num_scale</code> accordingly.</p>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-200m","title":"AICrossSim-CLM-200M","text":"<p>We applied Fully Sharded Data Parallel (FSDP) to the <code>AICrossSim-CLM-200M</code> training job to reduce memory usage, but this increases the training time.</p> <pre><code>batch_size=\"32\"\ndata_parallel_replicate=\"1\"\ndata_parallel_shard=\"2\"\npython run.py generate-cfg \\\n    --model_flavor 200M --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel_replicate} \\\n    --data_parallel_shard_degree ${data_parallel_shard} \\\n    --compile true \\\n    --save_path ./configs/tutorial-200M.yaml\n\nnum_gpus=\"2\" # 2 GPUs, 1 node\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-200M.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-200M</code> model on 2 NVIDIA H100 96GB GPUs for 6.5 hours.</p> <ul> <li>Training config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-200M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-200m</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-400m","title":"AICrossSim-CLM-400M","text":"<pre><code>batch_size=\"12\"\ndata_parallel_replicate=\"1\"\ndata_parallel_shard=\"8\"\npython run.py generate-cfg \\\n    --model_flavor 400M --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel_replicate} \\\n    --data_parallel_shard_degree ${data_parallel_shard} \\\n    --compile true \\\n    --save_path ./configs/tutorial-400M.yaml\n\nnum_gpus=\"8\" # 8 GPUs, 1 node\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-400M.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-400M</code> model on 8 NVIDIA A6000 GPUs for 21 hours.</p> <ul> <li>Training config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-200M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-400m</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#aicrosssim-clm-11b","title":"AICrossSim-CLM-1.1B","text":"<pre><code>batch_size=\"24\"\ndata_parallel_replicate=\"1\"\ndata_parallel_shard=\"8\"\npython run.py generate-cfg \\\n    --model_flavor 1.1B --batch_size ${batch_size} \\\n    --data_parallel_replicate_degree ${data_parallel_replicate} \\\n    --data_parallel_shard_degree ${data_parallel_shard} \\\n    --compile true \\\n    --save_path ./configs/tutorial-1.1B.yaml\n\nnum_gpus=\"8\" # 8 GPUs, 1 node\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain --config configs/tutorial-1.1B.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> <p>Our Pretraining Results</p> <p>We pretrained the <code>AICrossSim-CLM-1.1B</code> model on 8 NVIDIA H100 96GB GPUs for 33 hours.</p> <ul> <li>Training config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-1.1b.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/clm-1.1b</li> <li>We also stored the raw torchrun checkpoints in the HuggingFace repo in case we need to resume pretraining later. You can find them here</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#evaluation","title":"Evaluation","text":""},{"location":"01-model-training/llm-pretrain-and-eval/#pretraining-dataset-perplexity","title":"Pretraining Dataset Perplexity","text":"<p>We provide subcommands to evaluate the torchrun or HuggingFace checkpoints on the pretraining dataset.</p> <ul> <li>Torchrun checkpoint     <pre><code>python run.py eval pt-ppl \\\n    aixsim 60M \\\n    ./outputs/checkpoints/aixsim-60M/&lt;timestamp&gt;/&lt;step-xxx&gt;  # path to torchrun checkpoint\n</code></pre></li> <li>HuggingFace checkpoint     <pre><code>python run.py eval hf-ppl \\\n    AICrossSim/clm-60m  # path to local HuggingFace checkpoint or HuggingFace repo name\n</code></pre></li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#downstream-tasks","title":"Downstream Tasks","text":"<p>We leverage <code>lm-eval-harness</code> to evaluate the pretrained models on various tasks.</p> <p>For example, <pre><code>model_name=\"AICrossSim/clm-60m\" # Path to local HuggingFace checkpoint or HuggingFace repo name\npython run.py eval hf-lm-eval \\\n    ${model_name} \\\n    --tasks ['wikitext'] \\\n    --dtype float16\n</code></pre></p> <p>Try <code>--help</code> to see all the available arguments.</p> <pre><code>python run.py hf-lm-eval -h\n</code></pre> <p><code>lm-eval-harness</code></p> <p>Under the hood, the subcommand <code>hf-lm-eval</code> uses <code>lm-eval-harness</code>'s <code>simple_evaluate</code> function, thus it accepts several arguments of <code>simple_evaluate</code>:</p> <ul> <li><code>--tasks</code>: a list of tasks to evaluate on. The task names are the same as those in <code>lm-eval-harness</code>.</li> <li><code>--num_fewshot</code>: some downstream tasks support few-shot evaluation. Default <code>None</code> means default few-shot setting.</li> <li><code>--limit</code>: If <code>--limit</code> &gt; 1, it's the maximum number of examples to evaluate on, else it denotes the fraction of the dataset to evaluate on. Default <code>None</code> means evaluate on the entire dataset.</li> </ul>"},{"location":"01-model-training/llm-pretrain-and-eval/#simple-generation","title":"Simple Generation","text":"<p>We also provide a simple generation subcommand <code>hf-gen</code> to generate text using the pretrained models.</p> <pre><code>prompt=\"London is\"\nmax_new_tokens=\"100\"\ndo_sample=\"true\"\ntemperature=\"0.6\"\ntop_k=\"50\"\ntop_p=\"0.9\"\n\npython run.py hf-gen \\\n    --model_name AICrossSim/clm-60m \\\n    --prompt \"${prompt}\" \\\n    --max_new_tokens ${max_new_tokens} \\\n    --do_sample ${do_sample} \\\n    --temperature ${temperature} \\\n    --top_k ${top_k} \\\n    --top_p ${top_p}\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/","title":"Random Bitflip","text":"<p>This is tutorial on how to run post-bitflip evaluation on a pretrained checkpoint, and how to run a bitflip-aware pretraining from scratch.</p>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/#overview","title":"Overview","text":"<ul> <li>Post-bitflip evaluation loads a pretrained checkpoint from HuggingFace, applies bitflip transformation to the model, and evaluates the model on downstream tasks.<ul> <li>The entry point is at <code>experiments/llm-bitflip/transform/minimal.py</code>.</li> </ul> </li> <li>Bitflip-aware pretraining creates a randomly initialized model, applies bitflip transformation to the model, and pretrains the model on FineWeb-Edu.<ul> <li>The entry point is at <code>experiments/llm-bitflip/pretrain/run.py</code>.</li> </ul> </li> <li>To accelerate the emulation, we build custom triton kernels in <code>mase-triton</code>.<ul> <li>The random bitflip kernel is wrapped in function <code>mase_triton.random_bitflip.core.random_bitflip_fn</code>, which supports unique bitflip probability for the sign-exp bits and the mantissa bits. <code>random_bitflip_fn</code> also supports zeroing out outliers (and \"NaN\" values) by assigning a threshold.</li> <li>The random bitflip probability only supports a power of 0.5, e.g, <code>0.5</code>, <code>0.5^2</code>, <code>0.5^3</code>, etc. The kernel will automatically convert the probability to the nearest power of 0.5. Due to the limitation of the pseudo random number generation algorithm (Philox), the kernel only works for a random bitflip probability greater or equal to <code>0.5^-24=5.96-08</code>.</li> </ul> </li> </ul>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/#evaluation-of-post-training-bitflip-transform","title":"Evaluation of Post-Training Bitflip Transform","text":"<p>Environment Setup?</p> <p>If you have not set up environments, please follow the guidelines in Environment Setup.</p> <p>We offer minimal scripts to apply post-training bitflip transform on all linear layers (contributing to over 90% FLOPS in Transformers) in a HuggingFace pretrained model and evaluate the transformed model with <code>lm-eval-harness</code>.</p>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/#transform-evaluate-on-downstream-tasks","title":"Transform &amp; Evaluate on Downstream Tasks","text":"<pre><code>cd experiments/llm-bitflip/transform\n\nmodel_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\" # HuggingFace model name\nx_p_exp=null                                    # bitflip probability for the sign-exp bits of the activation. Null means no bitflip.\nw_p_exp=null                                    # bitflip probability for the sign-exp bits of the weight. Null means no bitflip.\nx_zero_out_t=\"100\"                              # threshold for zeroing out outliers (and \"NaN\" values) of the activation\nw_zero_out_t=\"1.25\"                             # threshold for zeroing out outliers (and \"NaN\" values) of the weight\nx_p_frac=$(bc &lt;&lt;&lt; \"scale=10; 0.5^10\")           # bitflip probability for the mantissa bits of the activation\nw_p_frac=$(bc &lt;&lt;&lt; \"scale=10; 0.5^10\")           # bitflip probability for the mantissa bits of the weight\npython minimal.py eval-bitflip \\\n    --model_name ${model_name} \\\n    --bitflip_config \"default\" \\\n    --default_bitflip_config.x_p_exp=${x_p_exp} \\\n    --default_bitflip_config.x_p_frac=${x_p_frac} \\\n    --default_bitflip_config.x_zero_out_t=${x_zero_out_t} \\\n    --default_bitflip_config.w_p_exp=${w_p_exp} \\\n    --default_bitflip_config.w_p_frac=${w_p_frac} \\\n    --default_bitflip_config.w_zero_out_t=${w_zero_out_t} \\\n    --tasks ['wikitext']\n</code></pre> <p>eval-bitflip</p> <p>This <code>eval-bitflip</code> subcommand also uses <code>lm-eval-harness</code>'s <code>simple_evaluate</code> function. Please refer to the evaluation section of LLM Pretraining &amp; Evaluation for more details.</p>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/#evaluate-the-original-model","title":"Evaluate the Original Model","text":"<p>You may want to compare the evaluation results of the bitflip model with the original model. You can do this by running the following command:</p> <pre><code>python minimal.py eval-ori \\\n    --model_name ${model_name} \\\n    --tasks ['wikitext']\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/#test-the-generation","title":"Test the Generation","text":"<p>We also offer a <code>hf-gen</code> script to</p>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/#simple-generation","title":"Simple Generation","text":"<p>We provide a simple generation subcommand <code>hf-gen</code> as well.</p> <pre><code>prompt=\"London is\"\nmax_new_tokens=\"100\"\ndo_sample=\"true\"\ntemperature=\"0.6\"\ntop_k=\"50\"\ntop_p=\"0.9\"\n\npython minimal.py hf-gen \\\n    AICrossSim/clm-60m \\\n    ${prompt} \\\n    --max_new_tokens ${max_new_tokens} \\\n    --do_sample ${do_sample} \\\n    --temperature ${temperature} \\\n    --top_k ${top_k} \\\n    --top_p ${top_p} \\\n    --bitflip_config \"default\" \\\n    --default_bitflip_config.x_p_exp=${x_p_exp} \\\n    --default_bitflip_config.x_p_frac=${x_p_frac} \\\n    --default_bitflip_config.x_zero_out_t=${x_zero_out_t} \\\n    --default_bitflip_config.w_p_exp=${w_p_exp} \\\n    --default_bitflip_config.w_p_frac=${w_p_frac} \\\n    --default_bitflip_config.w_zero_out_t=${w_zero_out_t}\n</code></pre> <p>Our Initial Experiments</p> <p>For our <code>AICrossSim/clm-1.1b</code>, we sweep <code>x_p_frac</code> and <code>w_p_frac</code> and observe how perplexity and generated texts changes.</p> <p>Here are some samples of the generated texts: link</p> <p>Notably, when the perplexity is increase by 1%, the generated text are consistent with the original text.</p>"},{"location":"02-model-behaviour-level-simulation/llm-bitflip/#bitflip-aware-pretraining","title":"Bitflip-Aware Pretraining","text":"<p>Similar to the pretraining of script of AICrossSim-CLM (<code>experiments/llm-bitflip/pretrain/run.py</code>), we offer a <code>experiments/llm-bitflip/pretrain/run.py</code> script to run bitflip-aware pretraining from scratch. The subcommands accepts the same arguments as <code>experiments/llm-bitflip/pretrain/run.py</code>, but with an additional argument for bitflip transform.</p> <ul> <li> <p>For example, we can run the following command to run a bitflip-aware pretraining for <code>AICrossSim-CLM-60M</code> on 2 H100 96GB GPUs.</p> <ol> <li> <p>Generate a training config with bitflip transform config.</p> <pre><code>cd experiments/llm-bitflip/pretrain\n\nbitflip_transform_config=\"./configs/meta/fc-only-w-a-exp-frac.yaml\"\npython run.py generate-cfg \\\n    ${bitflip_transform_config} \\\n    --model_arch \"aixsim\" \\\n    --model_flavor \"60M\" \\\n    --batch_size 48 \\\n    --data_parallel_replicate_degree 2\\\n    --data_parallel_shard_degree -1 \\\n    --token_num_scale 22 \\\n    --compile \"false\" \\\n    --save_path \"./configs/tutorial-60m.yaml\"\n</code></pre> </li> <li> <p>Run the pretraining with the generated config.</p> <pre><code>num_gpus=\"2\"\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\" STREAM_HF_DATA=\"1\" \\\ntorchrun --nproc_per_node=${num_gpus} --rdzv_backend c10d \\\n    --rdzv_endpoint=\"localhost:0\" --local-ranks-filter 0 \\\n    --role rank --tee 3 \\\n    run.py pretrain \\\n    --config configs/tutorial-60m.yaml \\\n    --metrics_args.enable_wandb false\n</code></pre> </li> <li> <p>Convert the checkpoint to HuggingFace format.</p> <pre><code>torchrun_ckpt_path=\"path/to/torchrun/checkpoint\"\noutput_dir=\"path/to/output/dir\"\npython run.py convert-ckpt pt2hf \\\n    \"aixsim\" \"60M\" \\\n    ${torchrun_ckpt_path} \\\n    ${output_dir}\n</code></pre> </li> </ol> <p>Our Bitflip-Aware Training Results of AICrossSim-CLM-60M</p> <p>We performed bitflip-aware pretraining on <code>AICrossSim-CLM-60M</code> on 2 H100 96GB GPUs for 2.5 hours.</p> <ul> <li>Train config: <code>experiments/llm-bitflip/pretrain/configs/aixsim-60M.yaml</code></li> <li>Wandb logs: link</li> <li>HuggingFace checkpoint: AICrossSim/bitflip-fc-clm-60m</li> </ul> <ul> <li> <p>Similarly, one can run bitflip-aware pretraining for other AICrossSim-CLM model sizes. Here we summarize our current bitflip-aware pretraining progress.</p> Model Environment Pretraining Time Training Config Wandb Logs HuggingFace Checkpoint 60M 2x H100 96GB 2.5 hours <code>configs/aixsim-60M.yaml</code> link AICrossSim/bitflip-fc-clm-60m 200M 2x H100 96GB 14.3 hours <code>configs/aixsim-200M.yaml</code> link AICrossSim/bitflip-fc-clm-200m 400M 6x A6000 48GB 33 hours <code>configs/aixsim-400M.yaml</code> link AICrossSim/bitflip-fc-clm-400m 1.1B 8x H200 141GB 51 hours <code>configs/aixsim-1.1B.yaml</code> link AICrossSim/bitflip-fc-clm-1.1b </li> </ul> </li> </ul>"},{"location":"02-model-behaviour-level-simulation/llm-optical-dev-guidelines/","title":"Development Guidelines for Transform-Aware LLM Training","text":"<p>In this codebase, we can support both transform-ware continual pretraining and pretraining from scratch. However, if the transform is too lossy, the model may not be able to learn effectively if trained from scratch. continual pretraining is recommended in this case.</p>"},{"location":"02-model-behaviour-level-simulation/llm-optical-dev-guidelines/#continual-pretraining","title":"Continual Pretraining","text":"<p>Example: Continual Pretraining with Simulated Optical Compute</p> <p>The example scripts can be found at <code>experiments/llm-optical-transformer/continual_pretraining</code></p> <p>HuggingFace <code>transformers</code>'s Trainer is used to perform continual pretraining on the converted/pretrained checkpoint on HuggingFace. Our pretrained AICrossSim/clm checkpoints can be found in this collection</p> <p>Here we use optical compute in the Optical Transformers (<code>OT</code>) paper as an example. You may follow the following steps to implement other new compute paradigms. To implement <code>OT</code>, we have a few key components you can find in <code>src/aixsim_models/optical_compute/optical_transformer</code>:</p> <ol> <li> <p>Simulated OT linear layer and matmul</p> <ul> <li>class <code>OpticalTransformerLinear</code> to simulate the linear layer. All the linear layers in the pretrained model will be replace by this linear layer except for <code>lm_head</code>.</li> <li>function <code>OpticalTransformerFunctions.quantized_matmul_fn</code> to simulate the matmul. The matmul is wrapped in <code>HFOpticalTransformerLlamaAttention</code> to simulate the Query-Key matmul and Attention-Value matmul.</li> </ul> <p>Kernels in <code>mase_triton.optical_compute</code></p> <ul> <li> <p>We use Triton instead of PyTorch API to implement <code>OpticalTransformerLinear</code> (essentially functional <code>ot_qlinear_fn</code>) and <code>OpticalTransformerFunctions.quantized_matmul_fn</code>, because for the method described in Optical Transformers, if we implement it using PyTorch built-in functions, the training will consume a lot of GPU memory and the training speed will be very slow. We implement Triton kernel mainly for saving GPU memory. If your simulation can be memory-effciently implemented using PyTorch built-in functions, you don't need to use Triton.</p> </li> <li> <p>HuggingFace <code>transformers</code>'s Trainer may not work with autotuned Triton kernels. This is why in <code>mase-triton</code>, the autotuning is disabled.</p> </li> </ul> </li> <li> <p>A pass to transform <code>LlamaForCausalLM</code>.</p> <p>We implement the function <code>transform_hf_model</code> to transform the model. Inside the function, there are two for loops, one for replacing attention layer with <code>HFOpticalTransformerLlamaAttention</code> to replace matmuls and the other for replacing linear layer with <code>OpticalTransformerLinear</code>.</p> </li> <li> <p>Transform config</p> <p>We use a YAML file to specify the transform config (<code>configs/default.yaml</code>). In <code>transform_hf_model</code>'s for loop, the <code>TransformConfigManager</code> uses the layer name to find the corresponding transform config.</p> </li> </ol> <p>With these two components, we can simply adapt HuggingFace's <code>run_clm.py</code> such that the model is transformed before training starts. The adapted <code>run_clm.py</code> can be found here.</p> <ul> <li> <p>In the adapted <code>run_clm.py</code>, we insert the following code snippet to transform the model before training starts:</p> <pre><code>    if model_args.transform_config is not None:\n        with open(model_args.transform_config, \"r\") as f:\n            transform_args = yaml.safe_load(f)\n        config_manager = TransformConfigManager(**transform_args)\n        transformed_layers = transform_hf_model(model, config_manager)\n        transform_histogram = make_transform_histogram(transformed_layers=transformed_layers)\n        logger.info(f\"\ud83d\udd0d Transformed layers:\\n{transform_histogram}\")\n    else:\n        logger.info(\"\u26a0\ufe0f No transform config file provided. Using the original model.\")\n</code></pre> </li> <li> <p>You may copy the adapted <code>run_clm.py</code> and replace the <code>OT</code> transform pass <code>transform_hf_model</code> with your own transform pass.</p> </li> </ul> <p>Then as shown in the <code>justfile</code>, we can launch the optical compute aware continual pretraining by:</p> <pre><code># This run uses small batch size and training steps for demonstration purpose.\npython run_clm.py \\\n    --model_name_or_path AICrossSim/clm-60m \\\n    --dataset_name HuggingFaceFW/fineweb-edu \\\n    --dataset_config_name \"sample-10BT\" \\\n    --per_device_train_batch_size 12 \\\n    --per_device_eval_batch_size 12 \\\n    --gradient_accumulation_steps 50 \\\n    --do_train \\\n    --report_to \"wandb\" \\\n    --learning_rate 5e-5 \\\n    --max_steps 100 \\\n    --save_strategy \"steps\" \\\n    --save_steps 500 \\\n    --save_total_limit 2 \\\n    --bf16 \\\n    --dataloader_num_workers 16 \\\n    --preprocessing_num_workers 32 \\\n    --tokenizer_name HuggingFaceTB/cosmo2-tokenizer \\\n    --output_dir ./output/test-clm-trainer \\\n    --transform_config ./configs/default.yaml \\\n    --seed 42\n</code></pre>"},{"location":"02-model-behaviour-level-simulation/llm-optical-dev-guidelines/#pretraining-from-scratch","title":"Pretraining from Scratch","text":"<p>We use torchtitan as the backend to pretrain transformed LLM from scratch. Please refer to <code>experiments/llm-optical-transformer/pretrain/run.py</code>.</p>"},{"location":"03-hardware-performance-simulation/underconstruction/","title":"README","text":"<p>The <code>Hardware-Performance Simulation</code> module is working in progress.</p>"}]}