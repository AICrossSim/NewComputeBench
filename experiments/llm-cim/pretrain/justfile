dev_run := "true"
tokenizer_name := "HuggingFaceTB/cosmo2-tokenizer"
# tokenizer_repo_path := "original" # this is for downloading the tiktoken tokenizer checkpoint in meta-llama/Llama-3.1-8B
model_arch := "aixsim"
model_flavor := "60M"
nnodes := "1"
ngpus := "2"
local_rank := "0"
seed := "42"
export HF_HUB_CACHE := "/data/hf_hub"
# training setup
batch_size := "48"
data_parallel_replicate_degree := "1"
data_parallel_shard_degree := "2"
tensor_parallel_degree := "1"
token_num_scale := "22.0"
torch_compiled := "true"
# eval
pt_ckpt := ""
hf_ckpt := ""
push_to_hub := ""
prompt := "The capital of England is"
do_sample := "true"
temperature := "1.0"
top_p := "1.0"
top_k := "10"
stream_hf_data := "1"


@check-vars:
    echo "========================================="
    echo "dev run: {{dev_run}}"
    echo "tokenizer name: {{tokenizer_name}}"
    echo "hf hub cache dir: $HF_HUB_CACHE"
    echo "model arch: {{model_arch}}"
    echo "model flavor: {{model_flavor}}"
    echo "nnodes: {{nnodes}}"
    echo "ngpus: {{ngpus}}"
    echo "local rank: {{local_rank}}"
    echo "batch size: {{batch_size}}"
    echo "data parallel replicate degree: {{data_parallel_replicate_degree}}"
    echo "data parallel shard degree: {{data_parallel_shard_degree}}"
    echo "tensor parallel degree: {{tensor_parallel_degree}}"
    echo "token num scale: {{token_num_scale}}"
    echo "torch compiled: {{torch_compiled}}"
    echo "pytorch checkpoint path: {{pt_ckpt}}"
    echo "hf checkpoint path: {{hf_ckpt}}"
    echo "push to hub: {{push_to_hub}}"
    echo "prompt: {{prompt}}"
    echo "do sample: {{do_sample}}"
    echo "temperature: {{temperature}}"
    echo "top p: {{top_p}}"
    echo "top k: {{top_k}}"
    echo "========================================="

# download-tokenizer: check-vars
#     python run.py download-hf-tokenizer {{tokenizer_name}} ./tokenizers/{{tokenizer_name}}

download-dataset: check-vars
    if [ "{{dev_run}}" = "false" ]; then \
        python run.py download-dataset fineweb-edu --allow_pattern "sample/100BT/**"; \
    fi

count-params: check-vars
    python run.py count-params {{model_arch}} {{model_flavor}} ./tokenizers/{{tokenizer_name}}

generate-cfg: check-vars
    python run.py generate-cfg --model_arch {{model_arch}} --model_flavor {{model_flavor}} --tokenizer_path {{tokenizer_name}} \
        --batch_size {{batch_size}} \
        --data_parallel_replicate_degree {{data_parallel_replicate_degree}} \
        --data_parallel_shard_degree {{data_parallel_shard_degree}} \
        --tensor_parallel_degree {{tensor_parallel_degree}} \
        --token_num_scale {{token_num_scale}} \
        --compile {{torch_compiled}} \
        --seed {{seed}}

pretrain: check-vars
    if [ "{{dev_run}}" = "true" ]; then \
        PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" STREAM_HF_DATA="1" \
        torchrun --nproc_per_node={{ngpus}} --rdzv_backend c10d \
        --rdzv_endpoint="localhost:0" --local-ranks-filter {{local_rank}} \
        --role rank --tee 3 \
        run.py pretrain --config configs/debug_model.yaml; \
    else \
        PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" STREAM_HF_DATA="{{stream_hf_data}}" \
        torchrun --nproc_per_node={{ngpus}} --rdzv_backend c10d \
        --rdzv_endpoint="localhost:0" --local-ranks-filter {{local_rank}} \
        --role rank --tee 3 \
        run.py pretrain --config configs/{{model_arch}}-{{model_flavor}}.yaml; \
    fi


pt-eval-ppl: check-vars
    if [ -z "{{pt_ckpt}}" ]; then \
        echo "⚠️ Please provide pt_ckpt"; \
    else \
        python run.py eval pt-ppl {{model_arch}} {{model_flavor}} \
            {{tokenizer_name}} {{pt_ckpt}} ; \
    fi


convert-ckpt: check-vars
    if [ -z "{{pt_ckpt}}" ]; then \
        echo "⚠️ Please provide pt_ckpt"; \
    elif [ -z "{{hf_ckpt}}" ]; then \
        echo "⚠️ Please provide hf_ckpt"; \
    else \
        python run.py convert-ckpt pt2hf {{model_arch}} {{model_flavor}} \
            {{tokenizer_name}} {{pt_ckpt}} {{hf_ckpt}} \
            --push_to_hub "{{push_to_hub}}" ; \
    fi


hf-eval-ppl: check-vars
    if [ -z "{{hf_ckpt}}" ]; then \
        echo "⚠️ Please provide hf_ckpt"; \
    else \
        python run.py eval hf-ppl {{hf_ckpt}}; \
    fi

generate-hf:
    if [ -z "{{hf_ckpt}}" ]; then \
        echo "⚠️ Please provide hf_ckpt"; \
    else \
        python run.py hf-gen {{hf_ckpt}} "{{prompt}}" --do_sample {{do_sample}} --temperature {{temperature}} --top_p {{top_p}} --top_k {{top_k}} ; \
    fi