dev_run := "true"
tokenizer_name := "HuggingFaceTB/cosmo2-tokenizer"
# tokenizer_repo_path := "original" # this is for downloading the tiktoken tokenizer checkpoint in meta-llama/Llama-3.1-8B
model_arch := "aixsim"
model_flavor := "60M"
nnodes := "1"
ngpus := "2"
local_rank := "0"
seed := "42"
export HF_HUB_CACHE := "/data/hf_hub"
# training setup
batch_size := "48"
data_parallel_replicate_degree := "1"
data_parallel_shard_degree := "2"
tensor_parallel_degree := "1"
token_num_scale := "22.0"
torch_compiled := "false"
# eval
pt_ckpt := ""
hf_ckpt := ""
push_to_hub := ""
prompt := "The capital of England is"
do_sample := "true"
temperature := "1.0"
top_p := "1.0"
top_k := "10"
stream_hf_data := "1"
# transform
meta_t_cfg := "./configs/meta/fc-only-w-only-frac-only.yaml"

@check-vars:
    echo "========================================="
    echo "dev run: {{dev_run}}"
    echo "tokenizer name: {{tokenizer_name}}"
    echo "hf hub cache dir: $HF_HUB_CACHE"
    echo "model arch: {{model_arch}}"
    echo "model flavor: {{model_flavor}}"
    echo "nnodes: {{nnodes}}"
    echo "ngpus: {{ngpus}}"
    echo "local rank: {{local_rank}}"
    echo "batch size: {{batch_size}}"
    echo "data parallel replicate degree: {{data_parallel_replicate_degree}}"
    echo "data parallel shard degree: {{data_parallel_shard_degree}}"
    echo "tensor parallel degree: {{tensor_parallel_degree}}"
    echo "token num scale: {{token_num_scale}}"
    echo "torch compiled: {{torch_compiled}}"
    echo "pytorch checkpoint path: {{pt_ckpt}}"
    echo "hf checkpoint path: {{hf_ckpt}}"
    echo "push to hub: {{push_to_hub}}"
    echo "prompt: {{prompt}}"
    echo "do sample: {{do_sample}}"
    echo "temperature: {{temperature}}"
    echo "top p: {{top_p}}"
    echo "top k: {{top_k}}"
    echo "meta transform config: {{meta_t_cfg}}"
    echo "========================================="


generate-cfg: check-vars
    python run.py generate-cfg {{meta_t_cfg}} --model_arch {{model_arch}} --model_flavor {{model_flavor}} --tokenizer_path {{tokenizer_name}} \
        --batch_size {{batch_size}} \
        --data_parallel_replicate_degree {{data_parallel_replicate_degree}} \
        --data_parallel_shard_degree {{data_parallel_shard_degree}} \
        --tensor_parallel_degree {{tensor_parallel_degree}} \
        --token_num_scale {{token_num_scale}} \
        --compile {{torch_compiled}} \
        --seed {{seed}}


pretrain: check-vars
    if [ "{{dev_run}}" = "true" ]; then \
        PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" STREAM_HF_DATA="1" \
        torchrun --nproc_per_node={{ngpus}} --rdzv_backend c10d \
        --rdzv_endpoint="localhost:0" --local-ranks-filter {{local_rank}} \
        --role rank --tee 3 \
        run.py pretrain --config configs/debug_model.yaml; \
    else \
        PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" STREAM_HF_DATA="{{stream_hf_data}}" \
        torchrun --nproc_per_node={{ngpus}} --rdzv_backend c10d \
        --rdzv_endpoint="localhost:0" --local-ranks-filter {{local_rank}} \
        --role rank --tee 3 \
        run.py pretrain --config configs/{{model_arch}}-{{model_flavor}}.yaml; \
    fi
